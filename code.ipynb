{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import base64\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim import corpora, models, similarities\n",
    "import random\n",
    "# print \"All reached here twice now....\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of rows in train dataset : ', 32512)\n",
      "['satwamuluhqgulamlrmvezuhqvkrpmletwulcitwskuhlemvtwamuluhiwiwenuhlrvimvqvkruhulenamuluhqgqvtwvimviwuhtwamuluhulqvkrenamcitwuhvipmpmqvuhskiwkrpmdfuhlrvimvskvikrpmqvuhskmvgzenleuhqvmvamuluhulenamuluhqvletwtwvipmpmgzleenamuhtwamuluhtwletwdfuhiwkrxeleentwxeuhpmqvuhtwiwmvamdfuhpkeztwamuluhvimvuhqvtwmkpmpmlelruhgztwtwskuhtwlrkrpmlruhpmuluhqvenuhtwyplepmxeuhenuhamypkrqvuhamulmvdfuhqvskentwamletwlrlrpmiwuhtwamul', 'twmkiwpmqvtwleuhsaiwsktwmvlelekramuhqvkruhtwskenezuhskvienuhqgulvienulqvvimvuhvienuhvimvuhulyptwbrtvkrqvuhtwamuluhsktwlrvienamypuhqvmvamguuhvgoaulamlrmvvibhpmuluhqvijulmvnkuhqgskkrpmiwenuhsktwskskenuhsaiwmvleenulvikriwpmmkvimvuhiguhvgqgulleentwamuhsaezuhqvqvtwiwtvuhskvisknkuhravidfpmvitwleuhvienmvypqvpmjeuhxepmuhlekrtwulenezenuhiwenmvypvimvmkpmlegzuhsktwulenletvtwiwtwypuhtwamuluhpmul', 'vidfpmskuhvilepmuluhtwtvuhulsovienamqvuhskiwmvamypuhtwamulsouhqgvienezuhtwamuluhamulmvdfuhsaiwulvitwiwpmmvmkuhlrvimviwlrlrkrleulqvuhqgiwlemvlruhtwamuluhsktwezentwleypqvuhsoqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhiguhqgiwiwmvdfuhqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhigsouhulqvvimvenlrenuhskentwamuhlekrpmsauhulmviwgzqvuhiwiwsoiguhlepmuhqgtwezuhezpmlexeuhxexepmuhskvienulxysouhuhragzqvenlelruhqvsoiwlemvlruhtwamul']\n",
      "['7', '3', '8']\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"/home/shringi/Downloads/offline_challenge/offline_challenge/xtrain_obfuscated.txt\")\n",
    "print(\"Number of rows in train dataset : \",train.shape[0])\n",
    "# print(train.shape)\n",
    "# print (train)\n",
    "# train.head()\n",
    "# train.values.tolist()\n",
    "lines = [line.rstrip('\\n') for line in open(\"/home/shringi/Downloads/offline_challenge/offline_challenge/xtrain_obfuscated.txt\")]\n",
    "# with open(\"/home/dupree/Downloads/offline_challenge/offline_challenge/xtrain_obfuscated.txt\") as f:\n",
    "#     lines = f.readlines()\n",
    "# random.shuffle(lines)\n",
    "train_data_x = lines[:26010]\n",
    "test_data_x = lines[26010:]\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open(\"/home/shringi/Downloads/offline_challenge/offline_challenge/ytrain.txt\")]\n",
    "\n",
    "# with open(\"/home/dupree/Downloads/offline_challenge/offline_challenge/ytrain.txt\") as f:\n",
    "#     lines = f.readlines()\n",
    "# random.shuffle(lines)\n",
    "train_data_y = lines[:26010]\n",
    "test_data_y = lines[26010:]\n",
    "\n",
    "# print len(train_data_x)\n",
    "# print len(test_data_x)\n",
    "# print len(train_data_y)\n",
    "# print len(test_data_y)\n",
    "\n",
    "print train_data_x[:3]\n",
    "print train_data_y[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "full_tfidf = tfidf_vec.fit_transform(train_data_x + test_data_x)\n",
    "train_tfidf = tfidf_vec.transform(train_data_x)\n",
    "test_tfidf = tfidf_vec.transform(test_data_x)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([26010, 3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "('Mean cv score : ', 3.1276120042373847)\n",
      "[[  1.19626796e-11   1.15195811e-02   7.93952199e-03 ...,   4.57994276e-01\n",
      "    1.11978317e-02   2.63548983e-08]\n",
      " [  3.96079970e-11   2.75968045e-02   1.21315664e-02 ...,   2.18864084e-01\n",
      "    3.50058437e-02   6.35578780e-08]\n",
      " [  2.98951027e-11   1.43037780e-01   1.24173516e-02 ...,   4.77800396e-01\n",
      "    5.73977827e-02   1.34920468e-07]\n",
      " ..., \n",
      " [  3.51506188e-11   1.70320447e-02   5.21268873e-02 ...,   4.42987899e-01\n",
      "    2.35565616e-02   1.28158293e-07]\n",
      " [  2.05111007e-11   3.40224544e-02   1.16163012e-02 ...,   5.03024509e-01\n",
      "    4.65865772e-02   7.52017707e-08]\n",
      " [  1.41125165e-11   4.58183303e-02   1.35308800e-02 ...,   2.05471229e-01\n",
      "    2.77940948e-02   2.27954862e-08]]\n"
     ]
    }
   ],
   "source": [
    "# print type(full_tfidf)\n",
    "# print full_tfidf.get_shape()\n",
    "# print train_tfidf.get_shape()\n",
    "# print test_tfidf.get_shape()\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "# for train_index, test_index in kf.split(train_data_x):\n",
    "# #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     print train_index, test_index\n",
    "# #     X_train, X_test = train_data_x[train_index], train_data_x[test_index]\n",
    "# #     y_train, y_test = train_data_y[train_index], train_data_y[test_index]\n",
    "#     print len(train_index), len(test_index)\n",
    "\n",
    "print train_tfidf.shape[0]\n",
    "\n",
    "train_data_y = np.asarray(train_data_y)\n",
    "for train_index, val_index in kf.split(train_data_x):\n",
    "    train_X, val_X = train_tfidf[train_index], train_tfidf[val_index]\n",
    "    train_y, val_y = train_data_y[train_index], train_data_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(train_X, train_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "#     pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print type(pred_test_y)\n",
    "#     print \"dev_X, val_X\", dev_X, val_X \n",
    "#     print \"dev_y, val_y\", dev_y, val_y\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "print pred_full_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n",
      "6503\n",
      "26010\n",
      "6503\n"
     ]
    }
   ],
   "source": [
    "train_data_y = lines[:26010]\n",
    "test_data_y = lines[26010:]\n",
    "\n",
    "print len(train_data_x)\n",
    "print len(test_data_x)\n",
    "print len(train_data_y)\n",
    "print len(test_data_y)\n",
    "\n",
    "\n",
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='char',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(train_data_x) + list(test_data_x))\n",
    "xtrain_tfv =  tfv.transform(train_data_x) \n",
    "xvalid_tfv = tfv.transform(test_data_x)\n",
    "# print type(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'a', 't', 'w', 'a', 'm', 'u', 'l', 'u', 'h', 'q', 'g', 'u', 'l', 'a', 'm', 'l', 'r', 'm', 'v', 'e', 'z', 'u', 'h', 'q', 'v', 'k', 'r', 'p', 'm', 'l', 'e', 't', 'w', 'u', 'l', 'c', 'i', 't', 'w', 's', 'k', 'u', 'h', 'l', 'e', 'm', 'v', 't', 'w', 'a', 'm', 'u', 'l', 'u', 'h', 'i', 'w', 'i', 'w', 'e', 'n', 'u', 'h', 'l', 'r', 'v', 'i', 'm', 'v', 'q', 'v', 'k', 'r', 'u', 'h', 'u', 'l', 'e', 'n', 'a', 'm', 'u', 'l', 'u', 'h', 'q', 'g', 'q', 'v', 't', 'w', 'v', 'i', 'm', 'v', 'i', 'w', 'u', 'h', 't', 'w', 'a', 'm', 'u', 'l', 'u', 'h', 'u', 'l', 'q', 'v', 'k', 'r', 'e', 'n', 'a', 'm', 'c', 'i', 't', 'w', 'u', 'h', 'v', 'i', 'p', 'm', 'p', 'm', 'q', 'v', 'u', 'h', 's', 'k', 'i', 'w', 'k', 'r', 'p', 'm', 'd', 'f', 'u', 'h', 'l', 'r', 'v', 'i', 'm', 'v', 's', 'k', 'v', 'i', 'k', 'r', 'p', 'm', 'q', 'v', 'u', 'h', 's', 'k', 'm', 'v', 'g', 'z', 'e', 'n', 'l', 'e', 'u', 'h', 'q', 'v', 'm', 'v', 'a', 'm', 'u', 'l', 'u', 'h', 'u', 'l', 'e', 'n', 'a', 'm', 'u', 'l', 'u', 'h', 'q', 'v', 'l', 'e', 't', 'w', 't', 'w', 'v', 'i', 'p', 'm', 'p', 'm', 'g', 'z', 'l', 'e', 'e', 'n', 'a', 'm', 'u', 'h', 't', 'w', 'a', 'm', 'u', 'l', 'u', 'h', 't', 'w', 'l', 'e', 't', 'w', 'd', 'f', 'u', 'h', 'i', 'w', 'k', 'r', 'x', 'e', 'l', 'e', 'e', 'n', 't', 'w', 'x', 'e', 'u', 'h', 'p', 'm', 'q', 'v', 'u', 'h', 't', 'w', 'i', 'w', 'm', 'v', 'a', 'm', 'd', 'f', 'u', 'h', 'p', 'k', 'e', 'z', 't', 'w', 'a', 'm', 'u', 'l', 'u', 'h', 'v', 'i', 'm', 'v', 'u', 'h', 'q', 'v', 't', 'w', 'm', 'k', 'p', 'm', 'p', 'm', 'l', 'e', 'l', 'r', 'u', 'h', 'g', 'z', 't', 'w', 't', 'w', 's', 'k', 'u', 'h', 't', 'w', 'l', 'r', 'k', 'r', 'p', 'm', 'l', 'r', 'u', 'h', 'p', 'm', 'u', 'l', 'u', 'h', 'q', 'v', 'e', 'n', 'u', 'h', 't', 'w', 'y', 'p', 'l', 'e', 'p', 'm', 'x', 'e', 'u', 'h', 'e', 'n', 'u', 'h', 'a', 'm', 'y', 'p', 'k', 'r', 'q', 'v', 'u', 'h', 'a', 'm', 'u', 'l', 'm', 'v', 'd', 'f', 'u', 'h', 'q', 'v', 's', 'k', 'e', 'n', 't', 'w', 'a', 'm', 'l', 'e', 't', 'w', 'l', 'r', 'l', 'r', 'p', 'm', 'i', 'w', 'u', 'h', 't', 'w', 'a', 'm', 'u', 'l']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_chars(s):\n",
    "        return list(s)\n",
    "print tokenize_chars(train_data_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "6503\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print type(predictions)\n",
    "print predictions.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "6503\n",
      "12\n",
      "[[ 0.0025959   0.03982532  0.02889083  0.02806657  0.05414063  0.63364265\n",
      "   0.05985316  0.01514786  0.00914334  0.09789767  0.01413626  0.0166598 ]\n",
      " [ 0.00702954  0.05273785  0.13342085  0.10302957  0.004141    0.12929616\n",
      "   0.00328851  0.02424679  0.46388523  0.00470743  0.06857282  0.00564426]\n",
      " [ 0.00289147  0.23618731  0.02724405  0.02272681  0.01831079  0.05084463\n",
      "   0.09632029  0.29937573  0.01623429  0.00240868  0.10995325  0.11750272]\n",
      " [ 0.00868383  0.00205862  0.00340142  0.00973028  0.01390498  0.02012879\n",
      "   0.03123459  0.84305834  0.00368125  0.01364261  0.01113381  0.03934149]]\n",
      "logloss: 0.999 \n",
      "0.368413520395% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print type(predictions)\n",
    "print predictions.shape[0]\n",
    "print predictions.shape[1]\n",
    "print predictions[:4]\n",
    "\n",
    "# metrics.log_loss(val_y, pred_val_y)\n",
    "# print (\"logloss: %0.3f \" % metrics.accuracy_score(np.asarray(test_data_y), np.asarray(predictions.round())))\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(test_data_y, predictions))\n",
    "# print (\"logloss: %0.3f \" % metrics.mean_absolute_error(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n",
    "# print str(math.exp(-(0.05))) + \"% is the accuracy percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.749 \n",
      "0.473040735892% is the accuracy percentage\n",
      "['3', '6', '5', '5', '5', '8', '6', '6', '3', '11', '9', '3', '0', '7', '4']\n",
      "[[  2.97318159e-06   1.10897887e-04   4.23011597e-04   1.74918032e-03\n",
      "    3.92525113e-03   9.76273714e-01   1.86638017e-04   6.25400868e-03\n",
      "    1.68136332e-05   1.08714859e-02   9.04665465e-05   9.55589715e-05]\n",
      " [  3.15605121e-07   3.45642926e-03   6.41779425e-02   1.09031521e-02\n",
      "    2.31007418e-10   4.40116894e-02   3.64839478e-12   3.65422701e-03\n",
      "    8.30714295e-01   2.09564446e-10   4.30549106e-02   2.70379590e-05]\n",
      " [  6.79973697e-10   2.29253167e-04   6.39394003e-06   8.59652701e-07\n",
      "    2.92258169e-06   6.26834645e-06   4.23104809e-02   5.49883469e-01\n",
      "    1.01142506e-03   4.37261857e-08   4.06219068e-01   3.29814989e-04]\n",
      " [  5.19388973e-10   5.18946744e-13   1.36493151e-07   6.98428786e-12\n",
      "    9.83708056e-07   1.62302814e-06   3.06163680e-03   9.96918831e-01\n",
      "    1.02302257e-06   1.05287600e-06   3.01322150e-10   1.47118904e-05]\n",
      " [  5.25343703e-08   1.36569094e-03   4.49568963e-06   1.52862804e-03\n",
      "    1.92664501e-04   7.42536448e-04   1.46318845e-03   9.59317821e-01\n",
      "    1.51359993e-04   1.26511211e-04   1.61537994e-02   1.89532514e-02]\n",
      " [  1.21646009e-05   8.28481827e-03   5.95357470e-09   2.82158043e-06\n",
      "    1.34150638e-07   4.65467941e-09   1.10128930e-10   2.93092242e-12\n",
      "    4.24327383e-03   2.11142657e-09   9.87456696e-01   7.82523848e-08]\n",
      " [  7.24164231e-10   9.53123617e-03   4.05519185e-02   1.19443965e-04\n",
      "    4.87916481e-04   2.74236787e-02   3.71581861e-02   1.28077377e-07\n",
      "    5.19242302e-01   2.26906665e-01   1.38480983e-01   9.75418437e-05]\n",
      " [  5.62932070e-11   1.88346675e-02   1.39378662e-02   3.20821865e-02\n",
      "    2.16518056e-02   9.37041150e-02   1.24176082e-06   5.05500635e-08\n",
      "    3.50200241e-01   3.92973258e-01   1.05638073e-04   7.65089293e-02]\n",
      " [  4.54428064e-03   8.60065491e-04   2.38009640e-02   1.03034959e-03\n",
      "    1.58053081e-08   9.46239169e-01   1.05039154e-05   2.28001111e-02\n",
      "    1.71297565e-05   5.16741450e-08   9.32691261e-08   6.97265607e-04]\n",
      " [  1.89946666e-09   1.45874148e-02   1.52569259e-02   9.18287966e-01\n",
      "    3.94737598e-08   4.29442467e-02   7.43021764e-09   3.47389025e-06\n",
      "    2.66193106e-03   1.08872740e-07   2.25054404e-03   4.00733946e-03]\n",
      " [  1.73856531e-06   2.42371319e-03   8.23240628e-02   9.39879093e-02\n",
      "    1.71864903e-01   1.99706803e-02   6.44014352e-03   1.15004643e-02\n",
      "    4.07443544e-02   3.63930991e-03   1.71308742e-02   5.49971847e-01]\n",
      " [  2.22589108e-07   1.01974163e-01   1.07785395e-01   9.43307878e-03\n",
      "    3.39291137e-02   2.13471752e-01   5.43142365e-03   3.09558395e-04\n",
      "    1.93457754e-01   3.30409951e-01   3.60443013e-03   1.93158713e-04]\n",
      " [  3.76237122e-01   7.06770068e-03   9.81800649e-04   3.89165752e-04\n",
      "    9.61582393e-05   2.24923002e-01   2.69634750e-01   4.25207386e-03\n",
      "    4.33642970e-04   3.93746946e-02   6.29117614e-06   7.66035983e-02]\n",
      " [  5.70705554e-09   3.18042687e-11   7.11983029e-02   3.69423921e-08\n",
      "    1.30051552e-04   7.67002019e-01   3.40659363e-05   1.81773895e-04\n",
      "    2.98152117e-11   1.59281098e-01   1.47592257e-07   2.17249840e-03]\n",
      " [  3.25379987e-10   1.97604978e-12   4.93222581e-10   1.05515794e-12\n",
      "    4.46034499e-09   1.70910987e-08   9.99999734e-01   5.10336476e-08\n",
      "    1.76762535e-07   1.45312074e-08   9.57233963e-10   2.65540689e-10]]\n"
     ]
    }
   ],
   "source": [
    "ctv = CountVectorizer(analyzer='char',\n",
    "            ngram_range=(1, 3))\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(train_data_x + test_data_x)\n",
    "xtrain_ctv =  ctv.transform(train_data_x) \n",
    "xvalid_ctv = ctv.transform(test_data_x)\n",
    "\n",
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n",
    "print test_data_y[:15]\n",
    "print predictions[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.473 \n",
      "0.229172949824% is the accuracy percentage\n",
      "logloss: 5.039 \n",
      "0.00647973566916% is the accuracy percentage\n",
      "Starting with fitting SVM\n",
      "logloss: 1.087 \n",
      "0.337154602202% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n",
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n",
    "\n",
    "\n",
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)\n",
    "\n",
    "print \"Starting with fitting SVM\"\n",
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shringi/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Came till multiclass logloss funtion....\n",
      "logloss: 0.772 \n",
      "0.461966812226% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print \"Came till multiclass logloss funtion....\"\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.245 \n",
      "0.287985656909% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.740 \n",
      "0.477161737492% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf CountVectorizer features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.772 \n",
      "0.461966812226% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "#Grid Searching on available models now\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "mll_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])\n",
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV] lr__penalty=l1, lr__C=0.1, svd__n_components=120 ................\n",
      "[CV] lr__penalty=l1, lr__C=0.1, svd__n_components=120 ................\n",
      "[CV] lr__penalty=l1, lr__C=0.1, svd__n_components=180 ................\n",
      "[CV] lr__penalty=l1, lr__C=0.1, svd__n_components=180 ................\n",
      "[CV] lr__penalty=l2, lr__C=0.1, svd__n_components=120 ................\n",
      "[CV] lr__penalty=l2, lr__C=0.1, svd__n_components=120 ................\n",
      "[CV] lr__penalty=l2, lr__C=0.1, svd__n_components=180 ................\n",
      "[CV] lr__penalty=l2, lr__C=0.1, svd__n_components=180 ................\n",
      "[CV]  lr__penalty=l1, lr__C=0.1, svd__n_components=120, score=-1.21435731963, total=  18.0s\n",
      "[CV] lr__penalty=l1, lr__C=1.0, svd__n_components=120 ................\n",
      "[CV]  lr__penalty=l1, lr__C=0.1, svd__n_components=120, score=-1.2179688342, total=  20.5s\n",
      "[CV] lr__penalty=l1, lr__C=1.0, svd__n_components=120 ................\n",
      "[CV]  lr__penalty=l2, lr__C=0.1, svd__n_components=120, score=-1.21985941361, total=  20.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   21.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] lr__penalty=l1, lr__C=1.0, svd__n_components=180 ................\n",
      "[CV]  lr__penalty=l2, lr__C=0.1, svd__n_components=120, score=-1.21748807569, total=  20.9s\n",
      "[CV] lr__penalty=l1, lr__C=1.0, svd__n_components=180 ................\n",
      "[CV]  lr__penalty=l2, lr__C=0.1, svd__n_components=180, score=-1.11942338388, total=  28.8s\n",
      "[CV] lr__penalty=l2, lr__C=1.0, svd__n_components=120 ................\n",
      "[CV]  lr__penalty=l2, lr__C=0.1, svd__n_components=180, score=-1.1192203202, total=  29.5s\n",
      "[CV] lr__penalty=l2, lr__C=1.0, svd__n_components=120 ................\n",
      "[CV]  lr__penalty=l1, lr__C=0.1, svd__n_components=180, score=-1.11908777712, total=  31.4s\n",
      "[CV] lr__penalty=l2, lr__C=1.0, svd__n_components=180 ................\n",
      "[CV]  lr__penalty=l1, lr__C=0.1, svd__n_components=180, score=-1.11796775141, total=  31.7s\n",
      "[CV] lr__penalty=l2, lr__C=1.0, svd__n_components=180 ................\n",
      "[CV]  lr__penalty=l1, lr__C=1.0, svd__n_components=120, score=-1.18973585731, total=  34.8s\n",
      "[CV] lr__penalty=l1, lr__C=10, svd__n_components=120 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   54.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__penalty=l1, lr__C=1.0, svd__n_components=120, score=-1.19737886789, total=  36.5s\n",
      "[CV] lr__penalty=l1, lr__C=10, svd__n_components=120 .................\n",
      "[CV]  lr__penalty=l2, lr__C=1.0, svd__n_components=120, score=-1.19154313811, total=  29.5s\n",
      "[CV] lr__penalty=l1, lr__C=10, svd__n_components=180 .................\n",
      "[CV]  lr__penalty=l2, lr__C=1.0, svd__n_components=120, score=-1.19691083949, total=  29.2s\n",
      "[CV] lr__penalty=l1, lr__C=10, svd__n_components=180 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.0min remaining:  1.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__penalty=l2, lr__C=1.0, svd__n_components=180, score=-1.09169415207, total=  43.8s\n",
      "[CV] lr__penalty=l2, lr__C=10, svd__n_components=120 .................\n",
      "[CV]  lr__penalty=l2, lr__C=1.0, svd__n_components=180, score=-1.09060819337, total=  44.4s\n",
      "[CV] lr__penalty=l2, lr__C=10, svd__n_components=120 .................\n",
      "[CV]  lr__penalty=l1, lr__C=10, svd__n_components=120, score=-1.18952913071, total=  36.9s\n",
      "[CV] lr__penalty=l2, lr__C=10, svd__n_components=180 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  1.5min remaining:   55.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__penalty=l1, lr__C=10, svd__n_components=120, score=-1.18685710648, total=  39.7s\n",
      "[CV] lr__penalty=l2, lr__C=10, svd__n_components=180 .................\n",
      "[CV]  lr__penalty=l1, lr__C=1.0, svd__n_components=180, score=-1.08525257076, total= 1.3min\n",
      "[CV]  lr__penalty=l2, lr__C=10, svd__n_components=120, score=-1.19535549985, total=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  1.8min remaining:   35.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__penalty=l2, lr__C=10, svd__n_components=120, score=-1.19675199186, total=  28.1s\n",
      "[CV]  lr__penalty=l1, lr__C=1.0, svd__n_components=180, score=-1.08731036011, total= 1.4min\n",
      "[CV]  lr__penalty=l2, lr__C=10, svd__n_components=180, score=-1.09167836891, total=  29.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.0min remaining:   17.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__penalty=l2, lr__C=10, svd__n_components=180, score=-1.09314138166, total=  25.3s\n",
      "[CV]  lr__penalty=l1, lr__C=10, svd__n_components=180, score=-1.09056821208, total= 1.1min\n",
      "[CV]  lr__penalty=l1, lr__C=10, svd__n_components=180, score=-1.09425929022, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -1.086\n",
      "Best parameters set:\n",
      "\tlr__C: 1.0\n",
      "\tlr__penalty: 'l1'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, np.asarray(train_data_y))  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] ............ nb__alpha=0.001, score=-1.22287939967, total=   0.1s\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] ............ nb__alpha=0.001, score=-1.21625014609, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] ............. nb__alpha=0.01, score=-1.23879005438, total=   0.1s\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ............. nb__alpha=0.01, score=-1.22638046816, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.6s remaining:    1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] .............. nb__alpha=0.1, score=-1.30567716648, total=   0.1s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] .............. nb__alpha=0.1, score=-1.29148632123, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.9s remaining:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] ................ nb__alpha=1, score=-1.62460238998, total=   0.1s\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ................ nb__alpha=1, score=-1.60767373943, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    1.2s remaining:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ............... nb__alpha=10, score=-2.90402435607, total=   0.1s\n",
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] ............... nb__alpha=10, score=-2.89400785782, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    1.4s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] .............. nb__alpha=100, score=-2.71141588116, total=   0.1s\n",
      "[CV] .............. nb__alpha=100, score=-2.71054921449, total=   0.1s\n",
      "Best score: -1.220\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "# This technique can be used to finetune xgboost or even multinomial naive bayes as below. We will use the tfidf data here:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, np.asarray(train_data_y))  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2571it [00:00, 25696.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/cpu:0', u'/gpu:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:22, 26724.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "# stop_words = stopwords.words('english')\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():  \n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n",
    "\n",
    "# Word Vectors\n",
    "# load the GloVe vectors in a dictionary:\n",
    "from tqdm import tqdm\n",
    "embeddings_index = {}\n",
    "f = open('/home/shringi/Downloads/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26010/26010 [00:05<00:00, 4711.44it/s]\n",
      "100%|██████████| 6503/6503 [00:01<00:00, 4756.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0327601   0.07798596 -0.08264437 ..., -0.00731602  0.01921969\n",
      "   0.04853879]\n",
      " [-0.0256834   0.07093357 -0.07748109 ..., -0.01181087  0.02081458\n",
      "   0.05021318]\n",
      " [-0.03204267  0.07462256 -0.08283966 ..., -0.01119007  0.0207957\n",
      "   0.04974462]\n",
      " [-0.03373921  0.06973441 -0.07722378 ..., -0.00863715  0.02233366\n",
      "   0.05069098]]\n",
      "[[-0.02916365  0.07641376 -0.08132176 ..., -0.01210064  0.01892132\n",
      "   0.05505707]\n",
      " [-0.02819736  0.07700168 -0.0813869  ..., -0.01140574  0.02110318\n",
      "   0.04664875]\n",
      " [-0.03109495  0.07669184 -0.08212794 ..., -0.00657532  0.020209\n",
      "   0.05389968]\n",
      " [-0.02792529  0.07169788 -0.07927565 ..., -0.009637    0.02120848\n",
      "   0.05436906]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "from nltk import word_tokenize\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = tokenize_chars(words)\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(450)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(train_data_x)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(test_data_x)]\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)\n",
    "\n",
    "print  xtrain_glove[:4]\n",
    "print  xvalid_glove[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.957 \n",
      "0.141304341394% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "# Fitting a xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.946 \n",
      "0.142898930209% is the accuracy percentage\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, train_data_y)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print (\"logloss: %0.3f \" % metrics.log_loss(np.asarray(test_data_y), predictions))\n",
    "print str(math.exp(-(metrics.log_loss(test_data_y, predictions)))) + \"% is the accuracy percentage\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/350\n",
      "26010/26010 [==============================] - 1s - loss: 2.2300 - val_loss: 1.9343\n",
      "Epoch 2/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.9497 - val_loss: 1.8554\n",
      "Epoch 3/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.8701 - val_loss: 1.8322\n",
      "Epoch 4/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.8259 - val_loss: 1.8097\n",
      "Epoch 5/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.8003 - val_loss: 1.7814\n",
      "Epoch 6/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7854 - val_loss: 1.7727\n",
      "Epoch 7/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7723 - val_loss: 1.7686\n",
      "Epoch 8/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7531 - val_loss: 1.7635\n",
      "Epoch 9/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7466 - val_loss: 1.7579\n",
      "Epoch 10/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7368 - val_loss: 1.7551\n",
      "Epoch 11/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7279 - val_loss: 1.7453\n",
      "Epoch 12/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7182 - val_loss: 1.7382\n",
      "Epoch 13/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7187 - val_loss: 1.7484\n",
      "Epoch 14/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.7071 - val_loss: 1.7408\n",
      "Epoch 15/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6979 - val_loss: 1.7383\n",
      "Epoch 16/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6942 - val_loss: 1.7290\n",
      "Epoch 17/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6871 - val_loss: 1.7272\n",
      "Epoch 18/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6831 - val_loss: 1.7322\n",
      "Epoch 19/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6815 - val_loss: 1.7405\n",
      "Epoch 20/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6696 - val_loss: 1.7342\n",
      "Epoch 21/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6683 - val_loss: 1.7263\n",
      "Epoch 22/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6613 - val_loss: 1.7275\n",
      "Epoch 23/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6600 - val_loss: 1.7217\n",
      "Epoch 24/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6536 - val_loss: 1.7293\n",
      "Epoch 25/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6437 - val_loss: 1.7193\n",
      "Epoch 26/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6466 - val_loss: 1.7167\n",
      "Epoch 27/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6320 - val_loss: 1.7305\n",
      "Epoch 28/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6326 - val_loss: 1.7326\n",
      "Epoch 29/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6306 - val_loss: 1.7222\n",
      "Epoch 30/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6245 - val_loss: 1.7240\n",
      "Epoch 31/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6196 - val_loss: 1.7228\n",
      "Epoch 32/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6185 - val_loss: 1.7209\n",
      "Epoch 33/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6130 - val_loss: 1.7241\n",
      "Epoch 34/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6097 - val_loss: 1.7266\n",
      "Epoch 35/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6096 - val_loss: 1.7194\n",
      "Epoch 36/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6053 - val_loss: 1.7270\n",
      "Epoch 37/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.6006 - val_loss: 1.7225\n",
      "Epoch 38/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5938 - val_loss: 1.7243\n",
      "Epoch 39/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5896 - val_loss: 1.7234\n",
      "Epoch 40/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5865 - val_loss: 1.7239\n",
      "Epoch 41/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5925 - val_loss: 1.7278\n",
      "Epoch 42/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5839 - val_loss: 1.7224\n",
      "Epoch 43/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5822 - val_loss: 1.7189\n",
      "Epoch 44/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5774 - val_loss: 1.7255\n",
      "Epoch 45/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5732 - val_loss: 1.7346\n",
      "Epoch 46/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5683 - val_loss: 1.7304\n",
      "Epoch 47/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5651 - val_loss: 1.7273\n",
      "Epoch 48/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5620 - val_loss: 1.7270\n",
      "Epoch 49/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5602 - val_loss: 1.7345\n",
      "Epoch 50/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5602 - val_loss: 1.7288\n",
      "Epoch 51/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5511 - val_loss: 1.7329\n",
      "Epoch 52/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5492 - val_loss: 1.7301\n",
      "Epoch 53/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5515 - val_loss: 1.7393\n",
      "Epoch 54/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5456 - val_loss: 1.7365\n",
      "Epoch 55/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5478 - val_loss: 1.7370\n",
      "Epoch 56/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5428 - val_loss: 1.7428\n",
      "Epoch 57/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5386 - val_loss: 1.7354\n",
      "Epoch 58/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5389 - val_loss: 1.7369\n",
      "Epoch 59/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5367 - val_loss: 1.7366\n",
      "Epoch 60/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5314 - val_loss: 1.7362\n",
      "Epoch 61/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5305 - val_loss: 1.7376\n",
      "Epoch 62/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5289 - val_loss: 1.7388\n",
      "Epoch 63/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5290 - val_loss: 1.7414\n",
      "Epoch 64/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5243 - val_loss: 1.7388\n",
      "Epoch 65/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5244 - val_loss: 1.7455\n",
      "Epoch 66/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5238 - val_loss: 1.7399\n",
      "Epoch 67/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5216 - val_loss: 1.7431\n",
      "Epoch 68/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5144 - val_loss: 1.7386\n",
      "Epoch 69/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5212 - val_loss: 1.7401\n",
      "Epoch 70/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5128 - val_loss: 1.7420\n",
      "Epoch 71/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5081 - val_loss: 1.7503\n",
      "Epoch 72/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5061 - val_loss: 1.7441\n",
      "Epoch 73/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5156 - val_loss: 1.7441\n",
      "Epoch 74/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5069 - val_loss: 1.7429\n",
      "Epoch 75/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5003 - val_loss: 1.7479\n",
      "Epoch 76/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5041 - val_loss: 1.7491\n",
      "Epoch 77/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5017 - val_loss: 1.7491\n",
      "Epoch 78/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.5002 - val_loss: 1.7498\n",
      "Epoch 79/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4951 - val_loss: 1.7477\n",
      "Epoch 80/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4867 - val_loss: 1.7523\n",
      "Epoch 81/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4960 - val_loss: 1.7462\n",
      "Epoch 82/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4931 - val_loss: 1.7550\n",
      "Epoch 83/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4853 - val_loss: 1.7493\n",
      "Epoch 84/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4908 - val_loss: 1.7534\n",
      "Epoch 85/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 1s - loss: 1.4867 - val_loss: 1.7537\n",
      "Epoch 86/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4914 - val_loss: 1.7581\n",
      "Epoch 87/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4810 - val_loss: 1.7574\n",
      "Epoch 88/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4850 - val_loss: 1.7556\n",
      "Epoch 89/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4829 - val_loss: 1.7637\n",
      "Epoch 90/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4816 - val_loss: 1.7620\n",
      "Epoch 91/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4761 - val_loss: 1.7556\n",
      "Epoch 92/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4837 - val_loss: 1.7564\n",
      "Epoch 93/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4731 - val_loss: 1.7524\n",
      "Epoch 94/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4717 - val_loss: 1.7555\n",
      "Epoch 95/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4789 - val_loss: 1.7614\n",
      "Epoch 96/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4695 - val_loss: 1.7558\n",
      "Epoch 97/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4688 - val_loss: 1.7589\n",
      "Epoch 98/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4659 - val_loss: 1.7648\n",
      "Epoch 99/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4688 - val_loss: 1.7599\n",
      "Epoch 100/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4630 - val_loss: 1.7656\n",
      "Epoch 101/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4623 - val_loss: 1.7644\n",
      "Epoch 102/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4612 - val_loss: 1.7670\n",
      "Epoch 103/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4601 - val_loss: 1.7680\n",
      "Epoch 104/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4551 - val_loss: 1.7659\n",
      "Epoch 105/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4529 - val_loss: 1.7650\n",
      "Epoch 106/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4587 - val_loss: 1.7644\n",
      "Epoch 107/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4523 - val_loss: 1.7691\n",
      "Epoch 108/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4525 - val_loss: 1.7637\n",
      "Epoch 109/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4480 - val_loss: 1.7698\n",
      "Epoch 110/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4504 - val_loss: 1.7641\n",
      "Epoch 111/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4536 - val_loss: 1.7741\n",
      "Epoch 112/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4441 - val_loss: 1.7696\n",
      "Epoch 113/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4442 - val_loss: 1.7759\n",
      "Epoch 114/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4506 - val_loss: 1.7657\n",
      "Epoch 115/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4507 - val_loss: 1.7699\n",
      "Epoch 116/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4481 - val_loss: 1.7713\n",
      "Epoch 117/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4472 - val_loss: 1.7627\n",
      "Epoch 118/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4361 - val_loss: 1.7646\n",
      "Epoch 119/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4416 - val_loss: 1.7600\n",
      "Epoch 120/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4368 - val_loss: 1.7666\n",
      "Epoch 121/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4381 - val_loss: 1.7699\n",
      "Epoch 122/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4362 - val_loss: 1.7701\n",
      "Epoch 123/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4442 - val_loss: 1.7697\n",
      "Epoch 124/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4407 - val_loss: 1.7701\n",
      "Epoch 125/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4382 - val_loss: 1.7705\n",
      "Epoch 126/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4254 - val_loss: 1.7713\n",
      "Epoch 127/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4402 - val_loss: 1.7763\n",
      "Epoch 128/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4299 - val_loss: 1.7679\n",
      "Epoch 129/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4374 - val_loss: 1.7758\n",
      "Epoch 130/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4319 - val_loss: 1.7730\n",
      "Epoch 131/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4341 - val_loss: 1.7752\n",
      "Epoch 132/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4324 - val_loss: 1.7702\n",
      "Epoch 133/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4221 - val_loss: 1.7747\n",
      "Epoch 134/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4323 - val_loss: 1.7745\n",
      "Epoch 135/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4251 - val_loss: 1.7755\n",
      "Epoch 136/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4221 - val_loss: 1.7823\n",
      "Epoch 137/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4217 - val_loss: 1.7788\n",
      "Epoch 138/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4268 - val_loss: 1.7721\n",
      "Epoch 139/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4213 - val_loss: 1.7747\n",
      "Epoch 140/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4224 - val_loss: 1.7800\n",
      "Epoch 141/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4235 - val_loss: 1.7832\n",
      "Epoch 142/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4149 - val_loss: 1.7781\n",
      "Epoch 143/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4252 - val_loss: 1.7804\n",
      "Epoch 144/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4176 - val_loss: 1.7780\n",
      "Epoch 145/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4152 - val_loss: 1.7737\n",
      "Epoch 146/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4151 - val_loss: 1.7725\n",
      "Epoch 147/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4182 - val_loss: 1.7801\n",
      "Epoch 148/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4058 - val_loss: 1.7815\n",
      "Epoch 149/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4107 - val_loss: 1.7841\n",
      "Epoch 150/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4107 - val_loss: 1.7858\n",
      "Epoch 151/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4156 - val_loss: 1.7869\n",
      "Epoch 152/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4104 - val_loss: 1.7818\n",
      "Epoch 153/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4058 - val_loss: 1.7786\n",
      "Epoch 154/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4174 - val_loss: 1.7782\n",
      "Epoch 155/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4061 - val_loss: 1.7836\n",
      "Epoch 156/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4037 - val_loss: 1.7863\n",
      "Epoch 157/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4025 - val_loss: 1.7851\n",
      "Epoch 158/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4088 - val_loss: 1.7938\n",
      "Epoch 159/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4056 - val_loss: 1.7858\n",
      "Epoch 160/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4044 - val_loss: 1.7842\n",
      "Epoch 161/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4032 - val_loss: 1.7865\n",
      "Epoch 162/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4013 - val_loss: 1.7829\n",
      "Epoch 163/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4014 - val_loss: 1.7860\n",
      "Epoch 164/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3955 - val_loss: 1.7930\n",
      "Epoch 165/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4024 - val_loss: 1.7921\n",
      "Epoch 166/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4060 - val_loss: 1.7772\n",
      "Epoch 167/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4049 - val_loss: 1.7861\n",
      "Epoch 168/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3957 - val_loss: 1.7894\n",
      "Epoch 169/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3947 - val_loss: 1.7876\n",
      "Epoch 170/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.4017 - val_loss: 1.7850\n",
      "Epoch 171/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3965 - val_loss: 1.7865\n",
      "Epoch 172/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3928 - val_loss: 1.7867\n",
      "Epoch 173/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3871 - val_loss: 1.7843\n",
      "Epoch 174/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3919 - val_loss: 1.7873\n",
      "Epoch 175/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3920 - val_loss: 1.7873\n",
      "Epoch 176/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3887 - val_loss: 1.7959\n",
      "Epoch 177/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3884 - val_loss: 1.7938\n",
      "Epoch 178/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3925 - val_loss: 1.7943\n",
      "Epoch 179/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3939 - val_loss: 1.7887\n",
      "Epoch 180/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3974 - val_loss: 1.7849\n",
      "Epoch 181/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3829 - val_loss: 1.7895\n",
      "Epoch 182/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3830 - val_loss: 1.7912\n",
      "Epoch 183/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3842 - val_loss: 1.7863\n",
      "Epoch 184/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3926 - val_loss: 1.7948\n",
      "Epoch 185/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3860 - val_loss: 1.7962\n",
      "Epoch 186/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3790 - val_loss: 1.7837\n",
      "Epoch 187/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3812 - val_loss: 1.7999\n",
      "Epoch 188/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3784 - val_loss: 1.7922\n",
      "Epoch 189/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3908 - val_loss: 1.7901\n",
      "Epoch 190/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3785 - val_loss: 1.7883\n",
      "Epoch 191/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3779 - val_loss: 1.7984\n",
      "Epoch 192/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3835 - val_loss: 1.7886\n",
      "Epoch 193/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3853 - val_loss: 1.7903\n",
      "Epoch 194/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3815 - val_loss: 1.7970\n",
      "Epoch 195/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3832 - val_loss: 1.7971\n",
      "Epoch 196/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3761 - val_loss: 1.7977\n",
      "Epoch 197/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3747 - val_loss: 1.7977\n",
      "Epoch 198/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3781 - val_loss: 1.8012\n",
      "Epoch 199/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3750 - val_loss: 1.7992\n",
      "Epoch 200/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3803 - val_loss: 1.7960\n",
      "Epoch 201/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3789 - val_loss: 1.7933\n",
      "Epoch 202/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3677 - val_loss: 1.8040\n",
      "Epoch 203/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3747 - val_loss: 1.7995\n",
      "Epoch 204/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3823 - val_loss: 1.7829\n",
      "Epoch 205/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3739 - val_loss: 1.7931\n",
      "Epoch 206/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3753 - val_loss: 1.7924\n",
      "Epoch 207/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3732 - val_loss: 1.7887\n",
      "Epoch 208/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3727 - val_loss: 1.7992\n",
      "Epoch 209/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3754 - val_loss: 1.7936\n",
      "Epoch 210/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3682 - val_loss: 1.7910\n",
      "Epoch 211/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3697 - val_loss: 1.7874\n",
      "Epoch 212/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3730 - val_loss: 1.7966\n",
      "Epoch 213/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3657 - val_loss: 1.7932\n",
      "Epoch 214/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3684 - val_loss: 1.7997\n",
      "Epoch 215/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3581 - val_loss: 1.7967\n",
      "Epoch 216/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3702 - val_loss: 1.7969\n",
      "Epoch 217/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3673 - val_loss: 1.7982\n",
      "Epoch 218/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3700 - val_loss: 1.7972\n",
      "Epoch 219/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3696 - val_loss: 1.8009\n",
      "Epoch 220/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3637 - val_loss: 1.7955\n",
      "Epoch 221/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3629 - val_loss: 1.8047\n",
      "Epoch 222/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3672 - val_loss: 1.7890\n",
      "Epoch 223/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3666 - val_loss: 1.7876\n",
      "Epoch 224/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3654 - val_loss: 1.7986\n",
      "Epoch 225/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3600 - val_loss: 1.8005\n",
      "Epoch 226/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3589 - val_loss: 1.8047\n",
      "Epoch 227/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3618 - val_loss: 1.8058\n",
      "Epoch 228/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3643 - val_loss: 1.8040\n",
      "Epoch 229/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3670 - val_loss: 1.7921\n",
      "Epoch 230/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3545 - val_loss: 1.7949\n",
      "Epoch 231/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3554 - val_loss: 1.8004\n",
      "Epoch 232/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3529 - val_loss: 1.8047\n",
      "Epoch 233/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3640 - val_loss: 1.7978\n",
      "Epoch 234/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3593 - val_loss: 1.7927\n",
      "Epoch 235/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3599 - val_loss: 1.7989\n",
      "Epoch 236/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3525 - val_loss: 1.7998\n",
      "Epoch 237/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3613 - val_loss: 1.7995\n",
      "Epoch 238/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3610 - val_loss: 1.8032\n",
      "Epoch 239/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3571 - val_loss: 1.8008\n",
      "Epoch 240/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3524 - val_loss: 1.7977\n",
      "Epoch 241/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3518 - val_loss: 1.8020\n",
      "Epoch 242/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3421 - val_loss: 1.8053\n",
      "Epoch 243/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3517 - val_loss: 1.8082\n",
      "Epoch 244/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3517 - val_loss: 1.8007\n",
      "Epoch 245/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3605 - val_loss: 1.8096\n",
      "Epoch 246/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3453 - val_loss: 1.8076\n",
      "Epoch 247/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3425 - val_loss: 1.8065\n",
      "Epoch 248/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3497 - val_loss: 1.8078\n",
      "Epoch 249/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3443 - val_loss: 1.8113\n",
      "Epoch 250/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3467 - val_loss: 1.8094\n",
      "Epoch 251/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 1s - loss: 1.3441 - val_loss: 1.8167\n",
      "Epoch 252/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3499 - val_loss: 1.8105\n",
      "Epoch 253/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3501 - val_loss: 1.8127\n",
      "Epoch 254/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3511 - val_loss: 1.8144\n",
      "Epoch 255/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3433 - val_loss: 1.8116\n",
      "Epoch 256/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3485 - val_loss: 1.8078\n",
      "Epoch 257/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3429 - val_loss: 1.8049\n",
      "Epoch 258/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3444 - val_loss: 1.8132\n",
      "Epoch 259/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3447 - val_loss: 1.8093\n",
      "Epoch 260/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3417 - val_loss: 1.8115\n",
      "Epoch 261/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3454 - val_loss: 1.8136\n",
      "Epoch 262/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3508 - val_loss: 1.8161\n",
      "Epoch 263/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3436 - val_loss: 1.8160\n",
      "Epoch 264/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3397 - val_loss: 1.8171\n",
      "Epoch 265/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3453 - val_loss: 1.8154\n",
      "Epoch 266/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3387 - val_loss: 1.8122\n",
      "Epoch 267/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3436 - val_loss: 1.8228\n",
      "Epoch 268/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3418 - val_loss: 1.8064\n",
      "Epoch 269/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3401 - val_loss: 1.8097\n",
      "Epoch 270/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3374 - val_loss: 1.8180\n",
      "Epoch 271/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3514 - val_loss: 1.8160\n",
      "Epoch 272/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3397 - val_loss: 1.8164\n",
      "Epoch 273/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3428 - val_loss: 1.8169\n",
      "Epoch 274/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3411 - val_loss: 1.8173\n",
      "Epoch 275/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3402 - val_loss: 1.8168\n",
      "Epoch 276/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3365 - val_loss: 1.8101\n",
      "Epoch 277/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3364 - val_loss: 1.8138\n",
      "Epoch 278/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3258 - val_loss: 1.8165\n",
      "Epoch 279/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3397 - val_loss: 1.8167\n",
      "Epoch 280/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3274 - val_loss: 1.8196\n",
      "Epoch 281/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3497 - val_loss: 1.8214\n",
      "Epoch 282/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3303 - val_loss: 1.8177\n",
      "Epoch 283/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3371 - val_loss: 1.8143\n",
      "Epoch 284/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3354 - val_loss: 1.8259\n",
      "Epoch 285/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3379 - val_loss: 1.8256\n",
      "Epoch 286/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3413 - val_loss: 1.8198\n",
      "Epoch 287/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3279 - val_loss: 1.8228\n",
      "Epoch 288/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3331 - val_loss: 1.8266\n",
      "Epoch 289/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3268 - val_loss: 1.8137\n",
      "Epoch 290/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3299 - val_loss: 1.8281\n",
      "Epoch 291/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3267 - val_loss: 1.8165\n",
      "Epoch 292/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3323 - val_loss: 1.8317\n",
      "Epoch 293/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3259 - val_loss: 1.8268\n",
      "Epoch 294/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3312 - val_loss: 1.8179\n",
      "Epoch 295/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3353 - val_loss: 1.8162\n",
      "Epoch 296/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3309 - val_loss: 1.8210\n",
      "Epoch 297/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3306 - val_loss: 1.8193\n",
      "Epoch 298/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3252 - val_loss: 1.8202\n",
      "Epoch 299/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3257 - val_loss: 1.8221\n",
      "Epoch 300/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3327 - val_loss: 1.8161\n",
      "Epoch 301/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3318 - val_loss: 1.8153\n",
      "Epoch 302/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3269 - val_loss: 1.8219\n",
      "Epoch 303/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3318 - val_loss: 1.8212\n",
      "Epoch 304/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3316 - val_loss: 1.8125\n",
      "Epoch 305/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3259 - val_loss: 1.8168\n",
      "Epoch 306/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3243 - val_loss: 1.8222\n",
      "Epoch 307/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3363 - val_loss: 1.8199\n",
      "Epoch 308/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3270 - val_loss: 1.8188\n",
      "Epoch 309/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3262 - val_loss: 1.8136\n",
      "Epoch 310/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3279 - val_loss: 1.8206\n",
      "Epoch 311/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3309 - val_loss: 1.8238\n",
      "Epoch 312/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3301 - val_loss: 1.8200\n",
      "Epoch 313/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3231 - val_loss: 1.8281\n",
      "Epoch 314/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3254 - val_loss: 1.8101\n",
      "Epoch 315/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3175 - val_loss: 1.8225\n",
      "Epoch 316/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3186 - val_loss: 1.8190\n",
      "Epoch 317/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3200 - val_loss: 1.8252\n",
      "Epoch 318/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3279 - val_loss: 1.8172\n",
      "Epoch 319/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3243 - val_loss: 1.8164\n",
      "Epoch 320/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3203 - val_loss: 1.8265\n",
      "Epoch 321/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3212 - val_loss: 1.8298\n",
      "Epoch 322/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3217 - val_loss: 1.8242\n",
      "Epoch 323/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3296 - val_loss: 1.8329\n",
      "Epoch 324/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3181 - val_loss: 1.8256\n",
      "Epoch 325/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3197 - val_loss: 1.8229\n",
      "Epoch 326/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3247 - val_loss: 1.8229\n",
      "Epoch 327/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3210 - val_loss: 1.8246\n",
      "Epoch 328/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3295 - val_loss: 1.8196\n",
      "Epoch 329/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3207 - val_loss: 1.8293\n",
      "Epoch 330/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3245 - val_loss: 1.8262\n",
      "Epoch 331/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3085 - val_loss: 1.8287\n",
      "Epoch 332/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3159 - val_loss: 1.8242\n",
      "Epoch 333/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3267 - val_loss: 1.8272\n",
      "Epoch 334/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3087 - val_loss: 1.8208\n",
      "Epoch 335/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3157 - val_loss: 1.8268\n",
      "Epoch 336/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3105 - val_loss: 1.8229\n",
      "Epoch 337/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3102 - val_loss: 1.8240\n",
      "Epoch 338/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3187 - val_loss: 1.8255\n",
      "Epoch 339/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3168 - val_loss: 1.8319\n",
      "Epoch 340/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3206 - val_loss: 1.8212\n",
      "Epoch 341/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3184 - val_loss: 1.8309\n",
      "Epoch 342/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3099 - val_loss: 1.8258\n",
      "Epoch 343/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3204 - val_loss: 1.8258\n",
      "Epoch 344/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3216 - val_loss: 1.8342\n",
      "Epoch 345/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3145 - val_loss: 1.8234\n",
      "Epoch 346/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3138 - val_loss: 1.8306\n",
      "Epoch 347/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3091 - val_loss: 1.8312\n",
      "Epoch 348/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3141 - val_loss: 1.8283\n",
      "Epoch 349/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3212 - val_loss: 1.8239\n",
      "Epoch 350/350\n",
      "26010/26010 [==============================] - 1s - loss: 1.3139 - val_loss: 1.8289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f375740dfd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning application -\n",
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical((train_data_y))\n",
    "yvalid_enc = np_utils.to_categorical((test_data_y))\n",
    "\n",
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=350, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26010, 300)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# print type(ytrain_enc)\n",
    "# print ytrain_enc.shape\n",
    "# print yvalid_enc.shape\n",
    "print xtrain_glove_scl.shape\n",
    "# print xtrain_glove_scl[0]\n",
    "# print xtrain_glove[0]\n",
    "print len(xtrain_glove[45])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING LSTM's now -----\n",
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None, char_level=True)\n",
    "max_len = 450\n",
    "\n",
    "token.fit_on_texts(list(train_data_x) + list(test_data_x))\n",
    "xtrain_seq = token.texts_to_sequences(train_data_x)\n",
    "xvalid_seq = token.texts_to_sequences(test_data_x)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n",
      "6503\n",
      "450\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 11  8  7 11  3\n",
      "  1  6  1  2 13 17  1  6 11  3  6 16  3  4  5 18  1  2 13  4 12 16 10  3  6\n",
      "  5  8  7  1  6 23  9  8  7 15 12  1  2  6  5  3  4  8  7 11  3  1  6  1  2\n",
      "  9  7  9  7  5 14  1  2  6 16  4  9  3  4 13  4 12 16  1  2  1  6  5 14 11\n",
      "  3  1  6  1  2 13 17 13  4  8  7  4  9  3  4  9  7  1  2  8  7 11  3  1  6\n",
      "  1  2  1  6 13  4 12 16  5 14 11  3 23  9  8  7  1  2  4  9 10  3 10  3 13\n",
      "  4  1  2 15 12  9  7 12 16 10  3 19 20  1  2  6 16  4  9  3  4 15 12  4  9\n",
      " 12 16 10  3 13  4  1  2 15 12  3  4 17 18  5 14  6  5  1  2 13  4  3  4 11\n",
      "  3  1  6  1  2  1  6  5 14 11  3  1  6  1  2 13  4  6  5  8  7  8  7  4  9\n",
      " 10  3 10  3 17 18  6  5  5 14 11  3  1  2  8  7 11  3  1  6  1  2  8  7  6\n",
      "  5  8  7 19 20  1  2  9  7 12 16 22  5  6  5  5 14  8  7 22  5  1  2 10  3\n",
      " 13  4  1  2  8  7  9  7  3  4 11  3 19 20  1  2 10 12  5 18  8  7 11  3  1\n",
      "  6  1  2  4  9  3  4  1  2 13  4  8  7  3 12 10  3 10  3  6  5  6 16  1  2\n",
      " 17 18  8  7  8  7 15 12  1  2  8  7  6 16 12 16 10  3  6 16  1  2 10  3  1\n",
      "  6  1  2 13  4  5 14  1  2  8  7 21 10  6  5 10  3 22  5  1  2  5 14  1  2\n",
      " 11  3 21 10 12 16 13  4  1  2 11  3  1  6  3  4 19 20  1  2 13  4 15 12  5\n",
      " 14  8  7 11  3  6  5  8  7  6 16  6 16 10  3  9  7  1  2  8  7 11  3  1  6]\n",
      "{'a': 11, 'c': 23, 'b': 26, 'e': 5, 'd': 19, 'g': 17, 'f': 20, 'i': 9, 'h': 2, 'k': 12, 'j': 25, 'm': 3, 'l': 6, 'o': 24, 'n': 14, 'q': 13, 'p': 10, 's': 15, 'r': 16, 'u': 1, 't': 8, 'w': 7, 'v': 4, 'y': 21, 'x': 22, 'z': 18}\n"
     ]
    }
   ],
   "source": [
    "print len(xtrain_seq)\n",
    "print len(xvalid_seq)\n",
    "\n",
    "print len(xvalid_pad[0])\n",
    "\n",
    "print xtrain_pad[0]\n",
    "print word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 170927.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.3735 - acc: 0.1413 - val_loss: 2.3552 - val_acc: 0.1610\n",
      "Epoch 2/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.3163 - acc: 0.1678 - val_loss: 2.2980 - val_acc: 0.1827\n",
      "Epoch 3/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2778 - acc: 0.1814 - val_loss: 2.2620 - val_acc: 0.1856\n",
      "Epoch 4/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2538 - acc: 0.1906 - val_loss: 2.2444 - val_acc: 0.1925\n",
      "Epoch 5/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2366 - acc: 0.1989 - val_loss: 2.2227 - val_acc: 0.1970\n",
      "Epoch 6/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2215 - acc: 0.1989 - val_loss: 2.2140 - val_acc: 0.2022\n",
      "Epoch 7/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2112 - acc: 0.2016 - val_loss: 2.2066 - val_acc: 0.2079\n",
      "Epoch 8/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2099 - acc: 0.2036 - val_loss: 2.2061 - val_acc: 0.2045\n",
      "Epoch 9/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2072 - acc: 0.2042 - val_loss: 2.2049 - val_acc: 0.2050\n",
      "Epoch 10/500\n",
      "26010/26010 [==============================] - 20s - loss: 2.2055 - acc: 0.2057 - val_loss: 2.2054 - val_acc: 0.2041\n",
      "Epoch 11/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2024 - acc: 0.2082 - val_loss: 2.2029 - val_acc: 0.2102\n",
      "Epoch 12/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.2006 - acc: 0.2078 - val_loss: 2.2021 - val_acc: 0.2105\n",
      "Epoch 13/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1977 - acc: 0.2097 - val_loss: 2.2006 - val_acc: 0.2094\n",
      "Epoch 14/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1987 - acc: 0.2107 - val_loss: 2.1987 - val_acc: 0.2081\n",
      "Epoch 15/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1975 - acc: 0.2100 - val_loss: 2.1975 - val_acc: 0.2104\n",
      "Epoch 16/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1957 - acc: 0.2126 - val_loss: 2.1976 - val_acc: 0.2087\n",
      "Epoch 17/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1944 - acc: 0.2121 - val_loss: 2.1967 - val_acc: 0.2061\n",
      "Epoch 18/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1912 - acc: 0.2152 - val_loss: 2.1921 - val_acc: 0.2097\n",
      "Epoch 19/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1860 - acc: 0.2143 - val_loss: 2.1618 - val_acc: 0.2253\n",
      "Epoch 20/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1682 - acc: 0.2185 - val_loss: 2.1376 - val_acc: 0.2331\n",
      "Epoch 21/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1560 - acc: 0.2256 - val_loss: 2.1274 - val_acc: 0.2497\n",
      "Epoch 22/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1434 - acc: 0.2331 - val_loss: 2.1221 - val_acc: 0.2528\n",
      "Epoch 23/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1395 - acc: 0.2338 - val_loss: 2.1201 - val_acc: 0.2553\n",
      "Epoch 24/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1371 - acc: 0.2388 - val_loss: 2.1141 - val_acc: 0.2559\n",
      "Epoch 25/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1277 - acc: 0.2423 - val_loss: 2.1141 - val_acc: 0.2530\n",
      "Epoch 26/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1285 - acc: 0.2443 - val_loss: 2.1096 - val_acc: 0.2593\n",
      "Epoch 27/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1201 - acc: 0.2444 - val_loss: 2.1056 - val_acc: 0.2594\n",
      "Epoch 28/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1171 - acc: 0.2467 - val_loss: 2.0994 - val_acc: 0.2611\n",
      "Epoch 29/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1118 - acc: 0.2486 - val_loss: 2.0935 - val_acc: 0.2643\n",
      "Epoch 30/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1066 - acc: 0.2517 - val_loss: 2.0884 - val_acc: 0.2648\n",
      "Epoch 31/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.1057 - acc: 0.2519 - val_loss: 2.0791 - val_acc: 0.2671\n",
      "Epoch 32/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0989 - acc: 0.2561 - val_loss: 2.0680 - val_acc: 0.2685\n",
      "Epoch 33/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0918 - acc: 0.2542 - val_loss: 2.0634 - val_acc: 0.2706\n",
      "Epoch 34/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0827 - acc: 0.2611 - val_loss: 2.0529 - val_acc: 0.2786\n",
      "Epoch 35/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0801 - acc: 0.2610 - val_loss: 2.0555 - val_acc: 0.2708\n",
      "Epoch 36/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0721 - acc: 0.2710 - val_loss: 2.0441 - val_acc: 0.2813\n",
      "Epoch 37/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0611 - acc: 0.2706 - val_loss: 2.0422 - val_acc: 0.2793\n",
      "Epoch 38/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0592 - acc: 0.2713 - val_loss: 2.0329 - val_acc: 0.2803\n",
      "Epoch 39/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0562 - acc: 0.2725 - val_loss: 2.0338 - val_acc: 0.2803\n",
      "Epoch 40/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0513 - acc: 0.2739 - val_loss: 2.0208 - val_acc: 0.2909\n",
      "Epoch 41/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0506 - acc: 0.2765 - val_loss: 2.0321 - val_acc: 0.2885\n",
      "Epoch 42/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0458 - acc: 0.2797 - val_loss: 2.0211 - val_acc: 0.2865\n",
      "Epoch 43/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0377 - acc: 0.2813 - val_loss: 2.0218 - val_acc: 0.2936\n",
      "Epoch 44/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0386 - acc: 0.2809 - val_loss: 2.0169 - val_acc: 0.2945\n",
      "Epoch 45/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0314 - acc: 0.2813 - val_loss: 2.0147 - val_acc: 0.2985\n",
      "Epoch 46/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0344 - acc: 0.2809 - val_loss: 2.0211 - val_acc: 0.2931\n",
      "Epoch 47/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0276 - acc: 0.2850 - val_loss: 2.0136 - val_acc: 0.2960\n",
      "Epoch 48/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0258 - acc: 0.2876 - val_loss: 1.9974 - val_acc: 0.2979\n",
      "Epoch 49/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0242 - acc: 0.2848 - val_loss: 1.9969 - val_acc: 0.3039\n",
      "Epoch 50/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0149 - acc: 0.2928 - val_loss: 2.0026 - val_acc: 0.2946\n",
      "Epoch 51/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0149 - acc: 0.2917 - val_loss: 1.9962 - val_acc: 0.2991\n",
      "Epoch 52/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0114 - acc: 0.2933 - val_loss: 1.9919 - val_acc: 0.3003\n",
      "Epoch 53/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0099 - acc: 0.2937 - val_loss: 1.9828 - val_acc: 0.3069\n",
      "Epoch 54/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0096 - acc: 0.2886 - val_loss: 1.9858 - val_acc: 0.3142\n",
      "Epoch 55/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0018 - acc: 0.2956 - val_loss: 1.9827 - val_acc: 0.3103\n",
      "Epoch 56/500\n",
      "26010/26010 [==============================] - 21s - loss: 2.0004 - acc: 0.2980 - val_loss: 1.9716 - val_acc: 0.3085\n",
      "Epoch 57/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9962 - acc: 0.2994 - val_loss: 1.9697 - val_acc: 0.3132\n",
      "Epoch 58/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9935 - acc: 0.3016 - val_loss: 1.9742 - val_acc: 0.3135\n",
      "Epoch 59/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9873 - acc: 0.3042 - val_loss: 1.9654 - val_acc: 0.3077\n",
      "Epoch 60/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9845 - acc: 0.3040 - val_loss: 1.9653 - val_acc: 0.3188\n",
      "Epoch 61/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9850 - acc: 0.3017 - val_loss: 1.9564 - val_acc: 0.3166\n",
      "Epoch 62/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9770 - acc: 0.3090 - val_loss: 1.9530 - val_acc: 0.3197\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 20s - loss: 1.9774 - acc: 0.3059 - val_loss: 1.9497 - val_acc: 0.3262\n",
      "Epoch 64/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9742 - acc: 0.3107 - val_loss: 1.9513 - val_acc: 0.3278\n",
      "Epoch 65/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.9687 - acc: 0.3120 - val_loss: 1.9409 - val_acc: 0.3254\n",
      "Epoch 66/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9674 - acc: 0.3120 - val_loss: 1.9324 - val_acc: 0.3314\n",
      "Epoch 67/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9549 - acc: 0.3191 - val_loss: 1.9248 - val_acc: 0.3365\n",
      "Epoch 68/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9495 - acc: 0.3193 - val_loss: 1.9155 - val_acc: 0.3392\n",
      "Epoch 69/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.9436 - acc: 0.3200 - val_loss: 1.8916 - val_acc: 0.3441\n",
      "Epoch 70/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9275 - acc: 0.3256 - val_loss: 1.8595 - val_acc: 0.3546\n",
      "Epoch 71/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.9011 - acc: 0.3324 - val_loss: 1.8208 - val_acc: 0.3683\n",
      "Epoch 72/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.8588 - acc: 0.3460 - val_loss: 1.7653 - val_acc: 0.3829\n",
      "Epoch 73/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7926 - acc: 0.3605 - val_loss: 1.7175 - val_acc: 0.3847\n",
      "Epoch 74/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7581 - acc: 0.3737 - val_loss: 1.6969 - val_acc: 0.3920\n",
      "Epoch 75/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7368 - acc: 0.3745 - val_loss: 1.6868 - val_acc: 0.3995\n",
      "Epoch 76/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7205 - acc: 0.3805 - val_loss: 1.6766 - val_acc: 0.4018\n",
      "Epoch 77/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7126 - acc: 0.3828 - val_loss: 1.6771 - val_acc: 0.3983\n",
      "Epoch 78/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.7072 - acc: 0.3870 - val_loss: 1.6665 - val_acc: 0.4046\n",
      "Epoch 79/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6974 - acc: 0.3901 - val_loss: 1.6626 - val_acc: 0.4034\n",
      "Epoch 80/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6938 - acc: 0.3934 - val_loss: 1.6571 - val_acc: 0.4063\n",
      "Epoch 81/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6844 - acc: 0.3937 - val_loss: 1.6521 - val_acc: 0.4055\n",
      "Epoch 82/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6738 - acc: 0.3968 - val_loss: 1.6303 - val_acc: 0.4187\n",
      "Epoch 83/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6725 - acc: 0.4032 - val_loss: 1.6266 - val_acc: 0.4215\n",
      "Epoch 84/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6707 - acc: 0.4001 - val_loss: 1.6441 - val_acc: 0.4149\n",
      "Epoch 85/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6555 - acc: 0.4066 - val_loss: 1.6212 - val_acc: 0.4243\n",
      "Epoch 86/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6496 - acc: 0.4057 - val_loss: 1.6092 - val_acc: 0.4369\n",
      "Epoch 87/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6481 - acc: 0.4119 - val_loss: 1.6043 - val_acc: 0.4336\n",
      "Epoch 88/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6437 - acc: 0.4132 - val_loss: 1.6032 - val_acc: 0.4355\n",
      "Epoch 89/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6420 - acc: 0.4185 - val_loss: 1.5983 - val_acc: 0.4381\n",
      "Epoch 90/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6320 - acc: 0.4200 - val_loss: 1.5942 - val_acc: 0.4336\n",
      "Epoch 91/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6384 - acc: 0.4173 - val_loss: 1.5962 - val_acc: 0.4410\n",
      "Epoch 92/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6243 - acc: 0.4269 - val_loss: 1.5841 - val_acc: 0.4443\n",
      "Epoch 93/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6126 - acc: 0.4295 - val_loss: 1.5792 - val_acc: 0.4475\n",
      "Epoch 94/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6157 - acc: 0.4269 - val_loss: 1.5741 - val_acc: 0.4564\n",
      "Epoch 95/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6124 - acc: 0.4317 - val_loss: 1.5705 - val_acc: 0.4533\n",
      "Epoch 96/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6065 - acc: 0.4338 - val_loss: 1.5625 - val_acc: 0.4619\n",
      "Epoch 97/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.6009 - acc: 0.4383 - val_loss: 1.5621 - val_acc: 0.4649\n",
      "Epoch 98/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5950 - acc: 0.4401 - val_loss: 1.5539 - val_acc: 0.4706\n",
      "Epoch 99/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5873 - acc: 0.4484 - val_loss: 1.5516 - val_acc: 0.4606\n",
      "Epoch 100/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5770 - acc: 0.4470 - val_loss: 1.5419 - val_acc: 0.4721\n",
      "Epoch 101/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5569 - acc: 0.4581 - val_loss: 1.5064 - val_acc: 0.4881\n",
      "Epoch 102/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5328 - acc: 0.4674 - val_loss: 1.4699 - val_acc: 0.4955\n",
      "Epoch 103/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.5026 - acc: 0.4807 - val_loss: 1.4648 - val_acc: 0.4939\n",
      "Epoch 104/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4939 - acc: 0.4795 - val_loss: 1.4405 - val_acc: 0.5059\n",
      "Epoch 105/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4772 - acc: 0.4860 - val_loss: 1.4277 - val_acc: 0.5119\n",
      "Epoch 106/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4644 - acc: 0.4840 - val_loss: 1.4184 - val_acc: 0.5119\n",
      "Epoch 107/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4604 - acc: 0.4889 - val_loss: 1.4210 - val_acc: 0.5090\n",
      "Epoch 108/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4473 - acc: 0.4915 - val_loss: 1.4133 - val_acc: 0.5121\n",
      "Epoch 109/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4494 - acc: 0.4906 - val_loss: 1.4043 - val_acc: 0.5170\n",
      "Epoch 110/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4418 - acc: 0.4972 - val_loss: 1.4003 - val_acc: 0.5128\n",
      "Epoch 111/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4378 - acc: 0.4971 - val_loss: 1.4019 - val_acc: 0.5128\n",
      "Epoch 112/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4310 - acc: 0.5023 - val_loss: 1.3965 - val_acc: 0.5173\n",
      "Epoch 113/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4269 - acc: 0.4973 - val_loss: 1.3934 - val_acc: 0.5171\n",
      "Epoch 114/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.4263 - acc: 0.4986 - val_loss: 1.3892 - val_acc: 0.5195\n",
      "Epoch 115/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4236 - acc: 0.5004 - val_loss: 1.3880 - val_acc: 0.5199\n",
      "Epoch 116/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4073 - acc: 0.5080 - val_loss: 1.3837 - val_acc: 0.5185\n",
      "Epoch 117/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4099 - acc: 0.5069 - val_loss: 1.3730 - val_acc: 0.5224\n",
      "Epoch 118/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4030 - acc: 0.5071 - val_loss: 1.3714 - val_acc: 0.5284\n",
      "Epoch 119/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.4003 - acc: 0.5096 - val_loss: 1.3802 - val_acc: 0.5161\n",
      "Epoch 120/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3992 - acc: 0.5104 - val_loss: 1.3665 - val_acc: 0.5235\n",
      "Epoch 121/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3980 - acc: 0.5104 - val_loss: 1.3592 - val_acc: 0.5307\n",
      "Epoch 122/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3833 - acc: 0.5151 - val_loss: 1.3591 - val_acc: 0.5264\n",
      "Epoch 123/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3863 - acc: 0.5132 - val_loss: 1.3394 - val_acc: 0.5350\n",
      "Epoch 124/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3784 - acc: 0.5135 - val_loss: 1.3316 - val_acc: 0.5436\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 21s - loss: 1.3634 - acc: 0.5188 - val_loss: 1.3206 - val_acc: 0.5410\n",
      "Epoch 126/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3555 - acc: 0.5230 - val_loss: 1.3145 - val_acc: 0.5373\n",
      "Epoch 127/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3477 - acc: 0.5251 - val_loss: 1.3094 - val_acc: 0.5401\n",
      "Epoch 128/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3327 - acc: 0.5236 - val_loss: 1.2872 - val_acc: 0.5530\n",
      "Epoch 129/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3305 - acc: 0.5303 - val_loss: 1.2921 - val_acc: 0.5441\n",
      "Epoch 130/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3194 - acc: 0.5302 - val_loss: 1.2806 - val_acc: 0.5467\n",
      "Epoch 131/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3224 - acc: 0.5288 - val_loss: 1.2964 - val_acc: 0.5424\n",
      "Epoch 132/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3032 - acc: 0.5373 - val_loss: 1.2622 - val_acc: 0.5559\n",
      "Epoch 133/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.3015 - acc: 0.5330 - val_loss: 1.2695 - val_acc: 0.5468\n",
      "Epoch 134/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2937 - acc: 0.5361 - val_loss: 1.2565 - val_acc: 0.5522\n",
      "Epoch 135/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2894 - acc: 0.5378 - val_loss: 1.2501 - val_acc: 0.5562\n",
      "Epoch 136/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2884 - acc: 0.5375 - val_loss: 1.2516 - val_acc: 0.5551\n",
      "Epoch 137/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2803 - acc: 0.5403 - val_loss: 1.2456 - val_acc: 0.5594\n",
      "Epoch 138/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2827 - acc: 0.5416 - val_loss: 1.2488 - val_acc: 0.5539\n",
      "Epoch 139/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2756 - acc: 0.5417 - val_loss: 1.2449 - val_acc: 0.5574\n",
      "Epoch 140/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2734 - acc: 0.5398 - val_loss: 1.2302 - val_acc: 0.5617\n",
      "Epoch 141/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.2704 - acc: 0.5413 - val_loss: 1.2316 - val_acc: 0.5584\n",
      "Epoch 142/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2649 - acc: 0.5479 - val_loss: 1.2316 - val_acc: 0.5644\n",
      "Epoch 143/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2618 - acc: 0.5457 - val_loss: 1.2319 - val_acc: 0.5499\n",
      "Epoch 144/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2502 - acc: 0.5523 - val_loss: 1.2205 - val_acc: 0.5660\n",
      "Epoch 145/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2549 - acc: 0.5501 - val_loss: 1.2098 - val_acc: 0.5696\n",
      "Epoch 146/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2499 - acc: 0.5496 - val_loss: 1.2123 - val_acc: 0.5690\n",
      "Epoch 147/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2401 - acc: 0.5556 - val_loss: 1.2065 - val_acc: 0.5691\n",
      "Epoch 148/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2326 - acc: 0.5566 - val_loss: 1.1964 - val_acc: 0.5717\n",
      "Epoch 149/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2392 - acc: 0.5527 - val_loss: 1.1902 - val_acc: 0.5702\n",
      "Epoch 150/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2289 - acc: 0.5566 - val_loss: 1.1947 - val_acc: 0.5733\n",
      "Epoch 151/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2122 - acc: 0.5616 - val_loss: 1.1920 - val_acc: 0.5720\n",
      "Epoch 152/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2220 - acc: 0.5602 - val_loss: 1.1864 - val_acc: 0.5793\n",
      "Epoch 153/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2152 - acc: 0.5612 - val_loss: 1.1959 - val_acc: 0.5659\n",
      "Epoch 154/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2190 - acc: 0.5579 - val_loss: 1.1821 - val_acc: 0.5783\n",
      "Epoch 155/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2208 - acc: 0.5594 - val_loss: 1.1846 - val_acc: 0.5696\n",
      "Epoch 156/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2108 - acc: 0.5623 - val_loss: 1.1811 - val_acc: 0.5751\n",
      "Epoch 157/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2118 - acc: 0.5636 - val_loss: 1.1786 - val_acc: 0.5745\n",
      "Epoch 158/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.2120 - acc: 0.5635 - val_loss: 1.1764 - val_acc: 0.5807\n",
      "Epoch 159/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1948 - acc: 0.5657 - val_loss: 1.1752 - val_acc: 0.5753\n",
      "Epoch 160/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1982 - acc: 0.5662 - val_loss: 1.1704 - val_acc: 0.5770\n",
      "Epoch 161/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1989 - acc: 0.5674 - val_loss: 1.1662 - val_acc: 0.5779\n",
      "Epoch 162/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1926 - acc: 0.5676 - val_loss: 1.1673 - val_acc: 0.5803\n",
      "Epoch 163/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1931 - acc: 0.5685 - val_loss: 1.1715 - val_acc: 0.5808\n",
      "Epoch 164/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1929 - acc: 0.5710 - val_loss: 1.1685 - val_acc: 0.5771\n",
      "Epoch 165/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1903 - acc: 0.5705 - val_loss: 1.1646 - val_acc: 0.5811\n",
      "Epoch 166/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1803 - acc: 0.5731 - val_loss: 1.1594 - val_acc: 0.5765\n",
      "Epoch 167/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1818 - acc: 0.5756 - val_loss: 1.1513 - val_acc: 0.5814\n",
      "Epoch 168/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1875 - acc: 0.5720 - val_loss: 1.1559 - val_acc: 0.5810\n",
      "Epoch 169/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1790 - acc: 0.5760 - val_loss: 1.1657 - val_acc: 0.5740\n",
      "Epoch 170/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1805 - acc: 0.5753 - val_loss: 1.1448 - val_acc: 0.5853\n",
      "Epoch 171/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1753 - acc: 0.5783 - val_loss: 1.1468 - val_acc: 0.5871\n",
      "Epoch 172/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1706 - acc: 0.5768 - val_loss: 1.1473 - val_acc: 0.5885\n",
      "Epoch 173/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1771 - acc: 0.5754 - val_loss: 1.1479 - val_acc: 0.5860\n",
      "Epoch 174/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1738 - acc: 0.5787 - val_loss: 1.1490 - val_acc: 0.5877\n",
      "Epoch 175/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1739 - acc: 0.5771 - val_loss: 1.1435 - val_acc: 0.5882\n",
      "Epoch 176/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1687 - acc: 0.5813 - val_loss: 1.1461 - val_acc: 0.5879\n",
      "Epoch 177/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.1610 - acc: 0.5825 - val_loss: 1.1388 - val_acc: 0.5894\n",
      "Epoch 178/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1680 - acc: 0.5769 - val_loss: 1.1401 - val_acc: 0.5910\n",
      "Epoch 179/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1602 - acc: 0.5834 - val_loss: 1.1457 - val_acc: 0.5868\n",
      "Epoch 180/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1610 - acc: 0.5804 - val_loss: 1.1398 - val_acc: 0.5837\n",
      "Epoch 181/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1510 - acc: 0.5840 - val_loss: 1.1319 - val_acc: 0.5923\n",
      "Epoch 182/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1565 - acc: 0.5819 - val_loss: 1.1338 - val_acc: 0.5897\n",
      "Epoch 183/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1540 - acc: 0.5843 - val_loss: 1.1333 - val_acc: 0.5865\n",
      "Epoch 184/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1452 - acc: 0.5864 - val_loss: 1.1240 - val_acc: 0.5948\n",
      "Epoch 185/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1500 - acc: 0.5842 - val_loss: 1.1292 - val_acc: 0.5888\n",
      "Epoch 186/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1500 - acc: 0.5849 - val_loss: 1.1258 - val_acc: 0.5926\n",
      "Epoch 187/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 21s - loss: 1.1520 - acc: 0.5826 - val_loss: 1.1294 - val_acc: 0.5900\n",
      "Epoch 188/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1469 - acc: 0.5868 - val_loss: 1.1265 - val_acc: 0.5885\n",
      "Epoch 189/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1481 - acc: 0.5860 - val_loss: 1.1260 - val_acc: 0.5882\n",
      "Epoch 190/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1429 - acc: 0.5856 - val_loss: 1.1247 - val_acc: 0.5943\n",
      "Epoch 191/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1420 - acc: 0.5846 - val_loss: 1.1332 - val_acc: 0.5960\n",
      "Epoch 192/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1400 - acc: 0.5879 - val_loss: 1.1220 - val_acc: 0.5974\n",
      "Epoch 193/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1334 - acc: 0.5901 - val_loss: 1.1254 - val_acc: 0.5933\n",
      "Epoch 194/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1357 - acc: 0.5917 - val_loss: 1.1274 - val_acc: 0.5916\n",
      "Epoch 195/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1290 - acc: 0.5907 - val_loss: 1.1195 - val_acc: 0.5973\n",
      "Epoch 196/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1348 - acc: 0.5931 - val_loss: 1.1241 - val_acc: 0.5950\n",
      "Epoch 197/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1325 - acc: 0.5910 - val_loss: 1.1150 - val_acc: 0.5970\n",
      "Epoch 198/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1357 - acc: 0.5904 - val_loss: 1.1148 - val_acc: 0.5974\n",
      "Epoch 199/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1335 - acc: 0.5932 - val_loss: 1.1159 - val_acc: 0.5908\n",
      "Epoch 200/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1236 - acc: 0.5939 - val_loss: 1.1168 - val_acc: 0.5943\n",
      "Epoch 201/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1233 - acc: 0.5985 - val_loss: 1.1277 - val_acc: 0.5885\n",
      "Epoch 202/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1196 - acc: 0.5950 - val_loss: 1.1214 - val_acc: 0.5896\n",
      "Epoch 203/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1280 - acc: 0.5928 - val_loss: 1.1184 - val_acc: 0.5970\n",
      "Epoch 204/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1275 - acc: 0.5945 - val_loss: 1.1127 - val_acc: 0.5974\n",
      "Epoch 205/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1202 - acc: 0.6010 - val_loss: 1.1060 - val_acc: 0.5986\n",
      "Epoch 206/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1151 - acc: 0.5990 - val_loss: 1.1055 - val_acc: 0.5994\n",
      "Epoch 207/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1176 - acc: 0.5973 - val_loss: 1.0987 - val_acc: 0.6006\n",
      "Epoch 208/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1185 - acc: 0.5961 - val_loss: 1.1036 - val_acc: 0.6020\n",
      "Epoch 209/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1222 - acc: 0.5967 - val_loss: 1.1024 - val_acc: 0.6017\n",
      "Epoch 210/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1097 - acc: 0.6006 - val_loss: 1.1003 - val_acc: 0.6040\n",
      "Epoch 211/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1095 - acc: 0.6018 - val_loss: 1.0946 - val_acc: 0.6071\n",
      "Epoch 212/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1080 - acc: 0.5983 - val_loss: 1.0963 - val_acc: 0.6076\n",
      "Epoch 213/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1146 - acc: 0.5984 - val_loss: 1.0999 - val_acc: 0.6059\n",
      "Epoch 214/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1043 - acc: 0.6047 - val_loss: 1.0959 - val_acc: 0.6037\n",
      "Epoch 215/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0998 - acc: 0.6048 - val_loss: 1.0916 - val_acc: 0.6068\n",
      "Epoch 216/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0995 - acc: 0.6052 - val_loss: 1.1027 - val_acc: 0.5965\n",
      "Epoch 217/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1012 - acc: 0.6060 - val_loss: 1.0974 - val_acc: 0.6014\n",
      "Epoch 218/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1028 - acc: 0.6038 - val_loss: 1.0911 - val_acc: 0.6097\n",
      "Epoch 219/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0951 - acc: 0.6096 - val_loss: 1.0857 - val_acc: 0.6108\n",
      "Epoch 220/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1013 - acc: 0.6057 - val_loss: 1.0838 - val_acc: 0.6093\n",
      "Epoch 221/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0967 - acc: 0.6027 - val_loss: 1.0929 - val_acc: 0.6097\n",
      "Epoch 222/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0964 - acc: 0.6072 - val_loss: 1.0896 - val_acc: 0.6171\n",
      "Epoch 223/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.1015 - acc: 0.6067 - val_loss: 1.0816 - val_acc: 0.6109\n",
      "Epoch 224/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.0890 - acc: 0.6139 - val_loss: 1.0888 - val_acc: 0.6060\n",
      "Epoch 225/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0928 - acc: 0.6118 - val_loss: 1.0796 - val_acc: 0.6103\n",
      "Epoch 226/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0915 - acc: 0.6054 - val_loss: 1.0747 - val_acc: 0.6162\n",
      "Epoch 227/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0875 - acc: 0.6072 - val_loss: 1.0844 - val_acc: 0.6076\n",
      "Epoch 228/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0874 - acc: 0.6076 - val_loss: 1.0749 - val_acc: 0.6148\n",
      "Epoch 229/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0896 - acc: 0.6085 - val_loss: 1.0838 - val_acc: 0.6096\n",
      "Epoch 230/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0902 - acc: 0.6116 - val_loss: 1.0784 - val_acc: 0.6091\n",
      "Epoch 231/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0878 - acc: 0.6109 - val_loss: 1.0770 - val_acc: 0.6140\n",
      "Epoch 232/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0849 - acc: 0.6089 - val_loss: 1.0878 - val_acc: 0.6062\n",
      "Epoch 233/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0823 - acc: 0.6094 - val_loss: 1.0761 - val_acc: 0.6137\n",
      "Epoch 234/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0754 - acc: 0.6131 - val_loss: 1.0687 - val_acc: 0.6194\n",
      "Epoch 235/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0812 - acc: 0.6118 - val_loss: 1.0804 - val_acc: 0.6099\n",
      "Epoch 236/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0776 - acc: 0.6151 - val_loss: 1.0773 - val_acc: 0.6122\n",
      "Epoch 237/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0786 - acc: 0.6120 - val_loss: 1.0761 - val_acc: 0.6119\n",
      "Epoch 238/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0752 - acc: 0.6113 - val_loss: 1.0714 - val_acc: 0.6165\n",
      "Epoch 239/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0709 - acc: 0.6145 - val_loss: 1.0708 - val_acc: 0.6129\n",
      "Epoch 240/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0803 - acc: 0.6158 - val_loss: 1.0654 - val_acc: 0.6228\n",
      "Epoch 241/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0711 - acc: 0.6155 - val_loss: 1.0680 - val_acc: 0.6196\n",
      "Epoch 242/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0666 - acc: 0.6165 - val_loss: 1.0660 - val_acc: 0.6199\n",
      "Epoch 243/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0727 - acc: 0.6182 - val_loss: 1.0695 - val_acc: 0.6145\n",
      "Epoch 244/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0775 - acc: 0.6145 - val_loss: 1.0687 - val_acc: 0.6165\n",
      "Epoch 245/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0680 - acc: 0.6174 - val_loss: 1.0685 - val_acc: 0.6189\n",
      "Epoch 246/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0737 - acc: 0.6183 - val_loss: 1.0707 - val_acc: 0.6143\n",
      "Epoch 247/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0626 - acc: 0.6173 - val_loss: 1.0653 - val_acc: 0.6173\n",
      "Epoch 248/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0627 - acc: 0.6196 - val_loss: 1.0653 - val_acc: 0.6154\n",
      "Epoch 249/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 21s - loss: 1.0721 - acc: 0.6148 - val_loss: 1.0672 - val_acc: 0.6162\n",
      "Epoch 250/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0637 - acc: 0.6163 - val_loss: 1.0625 - val_acc: 0.6183\n",
      "Epoch 251/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0613 - acc: 0.6195 - val_loss: 1.0649 - val_acc: 0.6182\n",
      "Epoch 252/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0681 - acc: 0.6159 - val_loss: 1.0663 - val_acc: 0.6160\n",
      "Epoch 253/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0614 - acc: 0.6180 - val_loss: 1.0616 - val_acc: 0.6211\n",
      "Epoch 254/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0603 - acc: 0.6220 - val_loss: 1.0705 - val_acc: 0.6194\n",
      "Epoch 255/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0539 - acc: 0.6234 - val_loss: 1.0612 - val_acc: 0.6202\n",
      "Epoch 256/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0627 - acc: 0.6193 - val_loss: 1.0732 - val_acc: 0.6131\n",
      "Epoch 257/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0615 - acc: 0.6202 - val_loss: 1.0587 - val_acc: 0.6236\n",
      "Epoch 258/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0574 - acc: 0.6223 - val_loss: 1.0558 - val_acc: 0.6206\n",
      "Epoch 259/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0622 - acc: 0.6185 - val_loss: 1.0559 - val_acc: 0.6213\n",
      "Epoch 260/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0520 - acc: 0.6220 - val_loss: 1.0566 - val_acc: 0.6200\n",
      "Epoch 261/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0488 - acc: 0.6218 - val_loss: 1.0535 - val_acc: 0.6206\n",
      "Epoch 262/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0597 - acc: 0.6208 - val_loss: 1.0555 - val_acc: 0.6206\n",
      "Epoch 263/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0548 - acc: 0.6213 - val_loss: 1.0540 - val_acc: 0.6263\n",
      "Epoch 264/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0442 - acc: 0.6243 - val_loss: 1.0464 - val_acc: 0.6246\n",
      "Epoch 265/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0500 - acc: 0.6240 - val_loss: 1.0546 - val_acc: 0.6234\n",
      "Epoch 266/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0492 - acc: 0.6237 - val_loss: 1.0442 - val_acc: 0.6277\n",
      "Epoch 267/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0534 - acc: 0.6243 - val_loss: 1.0458 - val_acc: 0.6280\n",
      "Epoch 268/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0504 - acc: 0.6250 - val_loss: 1.0528 - val_acc: 0.6239\n",
      "Epoch 269/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0503 - acc: 0.6225 - val_loss: 1.0431 - val_acc: 0.6317\n",
      "Epoch 270/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0424 - acc: 0.6272 - val_loss: 1.0405 - val_acc: 0.6302\n",
      "Epoch 271/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0470 - acc: 0.6250 - val_loss: 1.0473 - val_acc: 0.6271\n",
      "Epoch 272/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0397 - acc: 0.6266 - val_loss: 1.0415 - val_acc: 0.6309\n",
      "Epoch 273/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0439 - acc: 0.6271 - val_loss: 1.0395 - val_acc: 0.6331\n",
      "Epoch 274/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0483 - acc: 0.6285 - val_loss: 1.0411 - val_acc: 0.6322\n",
      "Epoch 275/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0401 - acc: 0.6266 - val_loss: 1.0449 - val_acc: 0.6260\n",
      "Epoch 276/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0446 - acc: 0.6261 - val_loss: 1.0435 - val_acc: 0.6256\n",
      "Epoch 277/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0354 - acc: 0.6302 - val_loss: 1.0400 - val_acc: 0.6297\n",
      "Epoch 278/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0343 - acc: 0.6280 - val_loss: 1.0485 - val_acc: 0.6277\n",
      "Epoch 279/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0397 - acc: 0.6296 - val_loss: 1.0394 - val_acc: 0.6322\n",
      "Epoch 280/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0365 - acc: 0.6320 - val_loss: 1.0339 - val_acc: 0.6312\n",
      "Epoch 281/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0372 - acc: 0.6293 - val_loss: 1.0379 - val_acc: 0.6314\n",
      "Epoch 282/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0313 - acc: 0.6283 - val_loss: 1.0365 - val_acc: 0.6297\n",
      "Epoch 283/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0275 - acc: 0.6289 - val_loss: 1.0329 - val_acc: 0.6342\n",
      "Epoch 284/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0386 - acc: 0.6310 - val_loss: 1.0425 - val_acc: 0.6280\n",
      "Epoch 285/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0323 - acc: 0.6322 - val_loss: 1.0327 - val_acc: 0.6294\n",
      "Epoch 286/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0334 - acc: 0.6289 - val_loss: 1.0265 - val_acc: 0.6352\n",
      "Epoch 287/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0306 - acc: 0.6313 - val_loss: 1.0288 - val_acc: 0.6306\n",
      "Epoch 288/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0366 - acc: 0.6313 - val_loss: 1.0316 - val_acc: 0.6312\n",
      "Epoch 289/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0297 - acc: 0.6327 - val_loss: 1.0309 - val_acc: 0.6297\n",
      "Epoch 290/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0251 - acc: 0.6309 - val_loss: 1.0258 - val_acc: 0.6322\n",
      "Epoch 291/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0321 - acc: 0.6307 - val_loss: 1.0309 - val_acc: 0.6279\n",
      "Epoch 292/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0270 - acc: 0.6333 - val_loss: 1.0283 - val_acc: 0.6317\n",
      "Epoch 293/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0255 - acc: 0.6341 - val_loss: 1.0252 - val_acc: 0.6376\n",
      "Epoch 294/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0302 - acc: 0.6334 - val_loss: 1.0262 - val_acc: 0.6348\n",
      "Epoch 295/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0290 - acc: 0.6326 - val_loss: 1.0237 - val_acc: 0.6332\n",
      "Epoch 296/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0299 - acc: 0.6341 - val_loss: 1.0267 - val_acc: 0.6316\n",
      "Epoch 297/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0249 - acc: 0.6319 - val_loss: 1.0244 - val_acc: 0.6303\n",
      "Epoch 298/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0186 - acc: 0.6373 - val_loss: 1.0204 - val_acc: 0.6352\n",
      "Epoch 299/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0208 - acc: 0.6319 - val_loss: 1.0215 - val_acc: 0.6400\n",
      "Epoch 300/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0190 - acc: 0.6361 - val_loss: 1.0188 - val_acc: 0.6343\n",
      "Epoch 301/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0152 - acc: 0.6370 - val_loss: 1.0217 - val_acc: 0.6332\n",
      "Epoch 302/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0249 - acc: 0.6322 - val_loss: 1.0205 - val_acc: 0.6369\n",
      "Epoch 303/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0198 - acc: 0.6336 - val_loss: 1.0209 - val_acc: 0.6346\n",
      "Epoch 304/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0258 - acc: 0.6359 - val_loss: 1.0164 - val_acc: 0.6345\n",
      "Epoch 305/500\n",
      "26010/26010 [==============================] - 20s - loss: 1.0121 - acc: 0.6368 - val_loss: 1.0186 - val_acc: 0.6408\n",
      "Epoch 306/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0185 - acc: 0.6397 - val_loss: 1.0181 - val_acc: 0.6336\n",
      "Epoch 307/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0156 - acc: 0.6382 - val_loss: 1.0175 - val_acc: 0.6369\n",
      "Epoch 308/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0141 - acc: 0.6384 - val_loss: 1.0161 - val_acc: 0.6323\n",
      "Epoch 309/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0116 - acc: 0.6373 - val_loss: 1.0184 - val_acc: 0.6332\n",
      "Epoch 310/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0166 - acc: 0.6416 - val_loss: 1.0291 - val_acc: 0.6305\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010/26010 [==============================] - 21s - loss: 1.0194 - acc: 0.6379 - val_loss: 1.0210 - val_acc: 0.6322\n",
      "Epoch 312/500\n",
      "26010/26010 [==============================] - 21s - loss: 1.0049 - acc: 0.6402 - val_loss: 1.0112 - val_acc: 0.6402\n",
      "Epoch 313/500\n",
      " 2560/26010 [=>............................] - ETA: 17s - loss: 1.0077 - acc: 0.6473"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e89b120ca648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=500, \n\u001b[0;32m---> 37\u001b[0;31m           verbose=1, validation_data=(xvalid_pad, yvalid_enc))\n\u001b[0m",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical((train_data_y))\n",
    "yvalid_enc = np_utils.to_categorical((test_data_y))        \n",
    "\n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=500, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 199728.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical((train_data_y))\n",
    "yvalid_enc = np_utils.to_categorical((test_data_y))\n",
    "\n",
    "embedding_matrix.shape\n",
    "# embedding_matrix[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/200\n",
      "26010/26010 [==============================] - 305s - loss: 2.3224 - val_loss: 2.2337\n",
      "Epoch 2/200\n",
      "26010/26010 [==============================] - 369s - loss: 2.2166 - val_loss: 2.1770\n",
      "Epoch 3/200\n",
      " 6016/26010 [=====>........................] - ETA: 230s - loss: 2.2030"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-762234feb6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=200, \n\u001b[0;32m---> 24\u001b[0;31m           verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n\u001b[0m",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=200, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.3901 - val_loss: 2.3528\n",
      "Epoch 2/100\n",
      "26010/26010 [==============================] - 231s - loss: 2.3114 - val_loss: 2.2849\n",
      "Epoch 3/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.2724 - val_loss: 2.1916\n",
      "Epoch 4/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.1454 - val_loss: 2.0725\n",
      "Epoch 5/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.0896 - val_loss: 2.0362\n",
      "Epoch 6/100\n",
      "26010/26010 [==============================] - 229s - loss: 2.0637 - val_loss: 2.0301\n",
      "Epoch 7/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.0619 - val_loss: 2.0359\n",
      "Epoch 8/100\n",
      "26010/26010 [==============================] - 230s - loss: 2.0557 - val_loss: 2.0349\n",
      "Epoch 9/100\n",
      "26010/26010 [==============================] - 231s - loss: 2.0554 - val_loss: 2.0332\n",
      "Epoch 10/100\n",
      "26010/26010 [==============================] - 231s - loss: 2.0567 - val_loss: 2.0317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f370c44bcd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/100\n",
      "26010/26010 [==============================] - 5s - loss: 2.3156 - acc: 0.1661 - val_loss: 2.1369 - val_acc: 0.2288\n",
      "Epoch 2/100\n",
      "26010/26010 [==============================] - 4s - loss: 1.9449 - acc: 0.3203 - val_loss: 1.8059 - val_acc: 0.3791\n",
      "Epoch 3/100\n",
      "26010/26010 [==============================] - 4s - loss: 1.5747 - acc: 0.4494 - val_loss: 1.4401 - val_acc: 0.4925\n",
      "Epoch 4/100\n",
      "26010/26010 [==============================] - 4s - loss: 1.2503 - acc: 0.5613 - val_loss: 1.2287 - val_acc: 0.5619\n",
      "Epoch 5/100\n",
      "26010/26010 [==============================] - 4s - loss: 1.0488 - acc: 0.6328 - val_loss: 1.1942 - val_acc: 0.5911\n",
      "Epoch 6/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.8775 - acc: 0.6954 - val_loss: 1.2926 - val_acc: 0.5763\n",
      "Epoch 7/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.7346 - acc: 0.7488 - val_loss: 1.2406 - val_acc: 0.6065\n",
      "Epoch 8/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.5993 - acc: 0.7957 - val_loss: 1.3695 - val_acc: 0.6119\n",
      "Epoch 9/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.4908 - acc: 0.8314 - val_loss: 1.3925 - val_acc: 0.6312\n",
      "Epoch 10/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.3998 - acc: 0.8627 - val_loss: 1.6235 - val_acc: 0.5960\n",
      "Epoch 11/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.3373 - acc: 0.8859 - val_loss: 1.7427 - val_acc: 0.6063\n",
      "Epoch 12/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2761 - acc: 0.9045 - val_loss: 1.8890 - val_acc: 0.6040\n",
      "Epoch 13/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2362 - acc: 0.9185 - val_loss: 1.9892 - val_acc: 0.6292\n",
      "Epoch 14/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2169 - acc: 0.9254 - val_loss: 2.1942 - val_acc: 0.6042\n",
      "Epoch 15/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2010 - acc: 0.9355 - val_loss: 2.3005 - val_acc: 0.6034\n",
      "Epoch 16/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1951 - acc: 0.9357 - val_loss: 2.4120 - val_acc: 0.6211\n",
      "Epoch 17/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1828 - acc: 0.9413 - val_loss: 2.6226 - val_acc: 0.5988\n",
      "Epoch 18/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1767 - acc: 0.9437 - val_loss: 2.7325 - val_acc: 0.6111\n",
      "Epoch 19/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1642 - acc: 0.9483 - val_loss: 2.6012 - val_acc: 0.6137\n",
      "Epoch 20/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1571 - acc: 0.9506 - val_loss: 2.7247 - val_acc: 0.6116\n",
      "Epoch 21/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1559 - acc: 0.9532 - val_loss: 2.6057 - val_acc: 0.6153\n",
      "Epoch 22/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1550 - acc: 0.9525 - val_loss: 2.8892 - val_acc: 0.6146\n",
      "Epoch 23/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1605 - acc: 0.9522 - val_loss: 2.7674 - val_acc: 0.6249\n",
      "Epoch 24/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1550 - acc: 0.9551 - val_loss: 3.0360 - val_acc: 0.6122\n",
      "Epoch 25/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1464 - acc: 0.9568 - val_loss: 3.1601 - val_acc: 0.6134\n",
      "Epoch 26/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1538 - acc: 0.9569 - val_loss: 3.0036 - val_acc: 0.6229\n",
      "Epoch 27/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1461 - acc: 0.9580 - val_loss: 3.0736 - val_acc: 0.6211\n",
      "Epoch 28/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1567 - acc: 0.9568 - val_loss: 3.1914 - val_acc: 0.6249\n",
      "Epoch 29/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1468 - acc: 0.9599 - val_loss: 3.2272 - val_acc: 0.6233\n",
      "Epoch 30/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1567 - acc: 0.9590 - val_loss: 3.3924 - val_acc: 0.6173\n",
      "Epoch 31/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1535 - acc: 0.9577 - val_loss: 3.6340 - val_acc: 0.6206\n",
      "Epoch 32/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1382 - acc: 0.9629 - val_loss: 3.5086 - val_acc: 0.6171\n",
      "Epoch 33/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1530 - acc: 0.9601 - val_loss: 3.3985 - val_acc: 0.6156\n",
      "Epoch 34/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1559 - acc: 0.9612 - val_loss: 3.3868 - val_acc: 0.6266\n",
      "Epoch 35/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1464 - acc: 0.9617 - val_loss: 3.5247 - val_acc: 0.6145\n",
      "Epoch 36/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1411 - acc: 0.9634 - val_loss: 3.8087 - val_acc: 0.6259\n",
      "Epoch 37/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1395 - acc: 0.9644 - val_loss: 3.5677 - val_acc: 0.6129\n",
      "Epoch 38/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1515 - acc: 0.9639 - val_loss: 3.4915 - val_acc: 0.6200\n",
      "Epoch 39/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1627 - acc: 0.9639 - val_loss: 3.5761 - val_acc: 0.6159\n",
      "Epoch 40/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1619 - acc: 0.9636 - val_loss: 3.6858 - val_acc: 0.6145\n",
      "Epoch 41/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1486 - acc: 0.9645 - val_loss: 3.7927 - val_acc: 0.6200\n",
      "Epoch 42/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1510 - acc: 0.9636 - val_loss: 3.6400 - val_acc: 0.6256\n",
      "Epoch 43/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1498 - acc: 0.9646 - val_loss: 4.0330 - val_acc: 0.6096\n",
      "Epoch 44/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1588 - acc: 0.9661 - val_loss: 4.1277 - val_acc: 0.6086\n",
      "Epoch 45/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1643 - acc: 0.9646 - val_loss: 4.2430 - val_acc: 0.5936\n",
      "Epoch 46/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1450 - acc: 0.9681 - val_loss: 3.8680 - val_acc: 0.6243\n",
      "Epoch 47/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1710 - acc: 0.9636 - val_loss: 3.8189 - val_acc: 0.6216\n",
      "Epoch 48/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1645 - acc: 0.9662 - val_loss: 4.0749 - val_acc: 0.6309\n",
      "Epoch 49/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1582 - acc: 0.9666 - val_loss: 4.1377 - val_acc: 0.6213\n",
      "Epoch 50/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1590 - acc: 0.9678 - val_loss: 3.9091 - val_acc: 0.6154\n",
      "Epoch 51/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1650 - acc: 0.9659 - val_loss: 4.2238 - val_acc: 0.6291\n",
      "Epoch 52/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1668 - acc: 0.9684 - val_loss: 4.0091 - val_acc: 0.6316\n",
      "Epoch 53/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1659 - acc: 0.9673 - val_loss: 4.1665 - val_acc: 0.6134\n",
      "Epoch 54/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1718 - acc: 0.9669 - val_loss: 3.8491 - val_acc: 0.6234\n",
      "Epoch 55/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1665 - acc: 0.9694 - val_loss: 4.2485 - val_acc: 0.6093\n",
      "Epoch 56/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1707 - acc: 0.9682 - val_loss: 4.4195 - val_acc: 0.6171\n",
      "Epoch 57/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1639 - acc: 0.9682 - val_loss: 4.4214 - val_acc: 0.6086\n",
      "Epoch 58/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1706 - acc: 0.9676 - val_loss: 4.2667 - val_acc: 0.6096\n",
      "Epoch 59/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1750 - acc: 0.9692 - val_loss: 3.8588 - val_acc: 0.6200\n",
      "Epoch 60/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1647 - acc: 0.9691 - val_loss: 4.4019 - val_acc: 0.6089\n",
      "Epoch 61/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1856 - acc: 0.9682 - val_loss: 4.3860 - val_acc: 0.6236\n",
      "Epoch 62/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1828 - acc: 0.9695 - val_loss: 4.4207 - val_acc: 0.6243\n",
      "Epoch 63/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1746 - acc: 0.9689 - val_loss: 4.1785 - val_acc: 0.6145\n",
      "Epoch 64/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1761 - acc: 0.9681 - val_loss: 4.5726 - val_acc: 0.6223\n",
      "Epoch 65/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1790 - acc: 0.9688 - val_loss: 4.3510 - val_acc: 0.6292\n",
      "Epoch 66/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2004 - acc: 0.9666 - val_loss: 4.7058 - val_acc: 0.6165\n",
      "Epoch 67/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.1996 - acc: 0.9677 - val_loss: 4.7222 - val_acc: 0.6122\n",
      "Epoch 68/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2001 - acc: 0.9684 - val_loss: 4.5724 - val_acc: 0.6289\n",
      "Epoch 69/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2007 - acc: 0.9672 - val_loss: 4.6823 - val_acc: 0.6200\n",
      "Epoch 70/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2078 - acc: 0.9670 - val_loss: 5.1196 - val_acc: 0.6025\n",
      "Epoch 71/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2184 - acc: 0.9675 - val_loss: 4.7352 - val_acc: 0.6176\n",
      "Epoch 72/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2122 - acc: 0.9682 - val_loss: 4.5974 - val_acc: 0.6294\n",
      "Epoch 73/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2081 - acc: 0.9694 - val_loss: 4.6808 - val_acc: 0.6256\n",
      "Epoch 74/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2144 - acc: 0.9683 - val_loss: 4.9742 - val_acc: 0.6140\n",
      "Epoch 75/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2149 - acc: 0.9689 - val_loss: 4.9365 - val_acc: 0.6083\n",
      "Epoch 76/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2009 - acc: 0.9709 - val_loss: 4.6735 - val_acc: 0.6286\n",
      "Epoch 77/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2181 - acc: 0.9677 - val_loss: 4.5982 - val_acc: 0.6213\n",
      "Epoch 78/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2046 - acc: 0.9700 - val_loss: 4.7147 - val_acc: 0.6265\n",
      "Epoch 79/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2234 - acc: 0.9689 - val_loss: 4.8859 - val_acc: 0.6251\n",
      "Epoch 80/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2292 - acc: 0.9680 - val_loss: 4.6813 - val_acc: 0.6179\n",
      "Epoch 81/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2298 - acc: 0.9702 - val_loss: 4.9021 - val_acc: 0.6256\n",
      "Epoch 82/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2409 - acc: 0.9663 - val_loss: 4.9195 - val_acc: 0.6208\n",
      "Epoch 83/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2401 - acc: 0.9691 - val_loss: 4.7207 - val_acc: 0.6256\n",
      "Epoch 84/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2258 - acc: 0.9696 - val_loss: 4.9016 - val_acc: 0.6276\n",
      "Epoch 85/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2293 - acc: 0.9701 - val_loss: 4.8717 - val_acc: 0.6254\n",
      "Epoch 86/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2190 - acc: 0.9704 - val_loss: 4.8482 - val_acc: 0.6205\n",
      "Epoch 87/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2496 - acc: 0.9688 - val_loss: 5.2041 - val_acc: 0.6180\n",
      "Epoch 88/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2305 - acc: 0.9706 - val_loss: 5.1157 - val_acc: 0.5923\n",
      "Epoch 89/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2511 - acc: 0.9684 - val_loss: 5.1296 - val_acc: 0.6189\n",
      "Epoch 90/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2416 - acc: 0.9699 - val_loss: 4.9523 - val_acc: 0.6297\n",
      "Epoch 91/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2487 - acc: 0.9694 - val_loss: 5.0346 - val_acc: 0.6183\n",
      "Epoch 92/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2609 - acc: 0.9682 - val_loss: 5.1625 - val_acc: 0.6153\n",
      "Epoch 93/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2435 - acc: 0.9715 - val_loss: 4.9191 - val_acc: 0.6262\n",
      "Epoch 94/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2533 - acc: 0.9710 - val_loss: 5.2077 - val_acc: 0.6233\n",
      "Epoch 95/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2779 - acc: 0.9686 - val_loss: 5.1755 - val_acc: 0.6322\n",
      "Epoch 96/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2907 - acc: 0.9667 - val_loss: 5.5317 - val_acc: 0.6106\n",
      "Epoch 97/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.3031 - acc: 0.9669 - val_loss: 5.1736 - val_acc: 0.6086\n",
      "Epoch 98/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2872 - acc: 0.9678 - val_loss: 5.4288 - val_acc: 0.6179\n",
      "Epoch 99/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2642 - acc: 0.9713 - val_loss: 5.3402 - val_acc: 0.6160\n",
      "Epoch 100/100\n",
      "26010/26010 [==============================] - 4s - loss: 0.2906 - acc: 0.9687 - val_loss: 5.2851 - val_acc: 0.6246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3704e1db90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a 1D convnet with existing GloVe features/vectors\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "2464/3000 [=======================>......] - ETA: 0s[6 1 8 8 4]\n",
      "(3000,)\n",
      "6\n",
      "1\n",
      "8\n",
      "8\n",
      "4\n",
      "10\n",
      "6\n",
      "6\n",
      "10\n",
      "8\n",
      "1\n",
      "7\n",
      "5\n",
      "10\n",
      "7\n",
      "10\n",
      "1\n",
      "6\n",
      "3\n",
      "8\n",
      "10\n",
      "11\n",
      "5\n",
      "6\n",
      "9\n",
      "7\n",
      "8\n",
      "2\n",
      "6\n",
      "6\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "8\n",
      "3\n",
      "10\n",
      "8\n",
      "1\n",
      "8\n",
      "6\n",
      "6\n",
      "9\n",
      "3\n",
      "1\n",
      "11\n",
      "1\n",
      "5\n",
      "7\n",
      "8\n",
      "7\n",
      "3\n",
      "7\n",
      "6\n",
      "8\n",
      "5\n",
      "9\n",
      "7\n",
      "11\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "4\n",
      "7\n",
      "3\n",
      "7\n",
      "6\n",
      "10\n",
      "3\n",
      "7\n",
      "4\n",
      "10\n",
      "6\n",
      "6\n",
      "7\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "5\n",
      "7\n",
      "1\n",
      "8\n",
      "5\n",
      "1\n",
      "10\n",
      "3\n",
      "8\n",
      "7\n",
      "8\n",
      "1\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "8\n",
      "8\n",
      "9\n",
      "8\n",
      "11\n",
      "1\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "7\n",
      "1\n",
      "4\n",
      "1\n",
      "7\n",
      "10\n",
      "5\n",
      "5\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "7\n",
      "2\n",
      "8\n",
      "1\n",
      "10\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "7\n",
      "10\n",
      "8\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "8\n",
      "3\n",
      "6\n",
      "7\n",
      "3\n",
      "7\n",
      "8\n",
      "6\n",
      "7\n",
      "10\n",
      "8\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "6\n",
      "6\n",
      "7\n",
      "2\n",
      "10\n",
      "1\n",
      "9\n",
      "11\n",
      "6\n",
      "7\n",
      "1\n",
      "0\n",
      "8\n",
      "11\n",
      "7\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "6\n",
      "5\n",
      "7\n",
      "7\n",
      "6\n",
      "1\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "1\n",
      "7\n",
      "8\n",
      "3\n",
      "8\n",
      "5\n",
      "7\n",
      "9\n",
      "1\n",
      "3\n",
      "6\n",
      "10\n",
      "7\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "2\n",
      "4\n",
      "3\n",
      "10\n",
      "1\n",
      "7\n",
      "7\n",
      "6\n",
      "7\n",
      "3\n",
      "2\n",
      "3\n",
      "11\n",
      "8\n",
      "4\n",
      "8\n",
      "2\n",
      "6\n",
      "9\n",
      "7\n",
      "7\n",
      "1\n",
      "10\n",
      "1\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "7\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "7\n",
      "1\n",
      "10\n",
      "8\n",
      "4\n",
      "10\n",
      "7\n",
      "8\n",
      "7\n",
      "7\n",
      "8\n",
      "5\n",
      "8\n",
      "3\n",
      "0\n",
      "7\n",
      "1\n",
      "7\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "3\n",
      "6\n",
      "1\n",
      "6\n",
      "8\n",
      "7\n",
      "4\n",
      "3\n",
      "7\n",
      "0\n",
      "7\n",
      "1\n",
      "6\n",
      "1\n",
      "5\n",
      "10\n",
      "6\n",
      "9\n",
      "9\n",
      "3\n",
      "5\n",
      "4\n",
      "2\n",
      "7\n",
      "8\n",
      "10\n",
      "10\n",
      "5\n",
      "10\n",
      "7\n",
      "4\n",
      "11\n",
      "4\n",
      "8\n",
      "5\n",
      "1\n",
      "2\n",
      "10\n",
      "10\n",
      "8\n",
      "0\n",
      "8\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "11\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "0\n",
      "11\n",
      "6\n",
      "3\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "1\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "4\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "10\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "4\n",
      "1\n",
      "8\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "3\n",
      "3\n",
      "8\n",
      "1\n",
      "1\n",
      "8\n",
      "6\n",
      "3\n",
      "3\n",
      "7\n",
      "1\n",
      "9\n",
      "10\n",
      "3\n",
      "8\n",
      "1\n",
      "7\n",
      "2\n",
      "6\n",
      "7\n",
      "10\n",
      "6\n",
      "5\n",
      "8\n",
      "6\n",
      "6\n",
      "7\n",
      "1\n",
      "8\n",
      "2\n",
      "7\n",
      "7\n",
      "3\n",
      "10\n",
      "8\n",
      "10\n",
      "4\n",
      "1\n",
      "8\n",
      "7\n",
      "3\n",
      "7\n",
      "6\n",
      "1\n",
      "1\n",
      "7\n",
      "0\n",
      "3\n",
      "7\n",
      "10\n",
      "2\n",
      "3\n",
      "7\n",
      "3\n",
      "7\n",
      "2\n",
      "8\n",
      "10\n",
      "1\n",
      "10\n",
      "10\n",
      "8\n",
      "11\n",
      "8\n",
      "8\n",
      "3\n",
      "7\n",
      "2\n",
      "8\n",
      "4\n",
      "2\n",
      "10\n",
      "7\n",
      "10\n",
      "3\n",
      "3\n",
      "5\n",
      "9\n",
      "4\n",
      "6\n",
      "1\n",
      "10\n",
      "7\n",
      "8\n",
      "1\n",
      "5\n",
      "7\n",
      "10\n",
      "0\n",
      "1\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "6\n",
      "10\n",
      "7\n",
      "11\n",
      "6\n",
      "3\n",
      "8\n",
      "7\n",
      "8\n",
      "7\n",
      "6\n",
      "10\n",
      "7\n",
      "8\n",
      "5\n",
      "3\n",
      "7\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "10\n",
      "0\n",
      "6\n",
      "10\n",
      "10\n",
      "3\n",
      "11\n",
      "3\n",
      "3\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "1\n",
      "1\n",
      "7\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "6\n",
      "7\n",
      "5\n",
      "10\n",
      "9\n",
      "7\n",
      "8\n",
      "3\n",
      "3\n",
      "7\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "7\n",
      "7\n",
      "6\n",
      "5\n",
      "10\n",
      "1\n",
      "6\n",
      "1\n",
      "8\n",
      "8\n",
      "6\n",
      "7\n",
      "3\n",
      "4\n",
      "3\n",
      "9\n",
      "8\n",
      "10\n",
      "3\n",
      "6\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "1\n",
      "5\n",
      "4\n",
      "9\n",
      "10\n",
      "7\n",
      "7\n",
      "4\n",
      "6\n",
      "8\n",
      "7\n",
      "11\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "3\n",
      "3\n",
      "7\n",
      "6\n",
      "5\n",
      "7\n",
      "1\n",
      "1\n",
      "8\n",
      "1\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "2\n",
      "3\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "7\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "10\n",
      "6\n",
      "5\n",
      "7\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "8\n",
      "4\n",
      "10\n",
      "3\n",
      "8\n",
      "7\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "1\n",
      "1\n",
      "1\n",
      "10\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "10\n",
      "10\n",
      "1\n",
      "7\n",
      "1\n",
      "4\n",
      "6\n",
      "8\n",
      "3\n",
      "1\n",
      "1\n",
      "8\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "5\n",
      "8\n",
      "3\n",
      "4\n",
      "11\n",
      "6\n",
      "2\n",
      "1\n",
      "10\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "10\n",
      "7\n",
      "11\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "8\n",
      "7\n",
      "4\n",
      "7\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "5\n",
      "6\n",
      "8\n",
      "6\n",
      "10\n",
      "9\n",
      "3\n",
      "11\n",
      "7\n",
      "3\n",
      "10\n",
      "3\n",
      "10\n",
      "10\n",
      "0\n",
      "7\n",
      "3\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "3\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "9\n",
      "6\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "7\n",
      "3\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "2\n",
      "1\n",
      "1\n",
      "10\n",
      "6\n",
      "7\n",
      "11\n",
      "7\n",
      "1\n",
      "10\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "8\n",
      "8\n",
      "5\n",
      "6\n",
      "1\n",
      "6\n",
      "5\n",
      "6\n",
      "7\n",
      "7\n",
      "0\n",
      "3\n",
      "10\n",
      "6\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "5\n",
      "3\n",
      "9\n",
      "6\n",
      "4\n",
      "2\n",
      "4\n",
      "7\n",
      "6\n",
      "7\n",
      "1\n",
      "9\n",
      "4\n",
      "10\n",
      "7\n",
      "6\n",
      "1\n",
      "9\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "10\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "8\n",
      "6\n",
      "5\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "6\n",
      "2\n",
      "8\n",
      "1\n",
      "0\n",
      "8\n",
      "6\n",
      "10\n",
      "2\n",
      "3\n",
      "6\n",
      "7\n",
      "7\n",
      "1\n",
      "5\n",
      "6\n",
      "1\n",
      "4\n",
      "8\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "1\n",
      "5\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "6\n",
      "7\n",
      "1\n",
      "7\n",
      "4\n",
      "10\n",
      "5\n",
      "9\n",
      "6\n",
      "11\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "6\n",
      "10\n",
      "3\n",
      "4\n",
      "1\n",
      "6\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "10\n",
      "1\n",
      "1\n",
      "1\n",
      "9\n",
      "10\n",
      "1\n",
      "7\n",
      "9\n",
      "10\n",
      "3\n",
      "3\n",
      "7\n",
      "6\n",
      "5\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "3\n",
      "6\n",
      "3\n",
      "8\n",
      "6\n",
      "8\n",
      "2\n",
      "8\n",
      "7\n",
      "6\n",
      "3\n",
      "5\n",
      "7\n",
      "8\n",
      "3\n",
      "10\n",
      "7\n",
      "8\n",
      "1\n",
      "6\n",
      "7\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "10\n",
      "4\n",
      "4\n",
      "6\n",
      "8\n",
      "4\n",
      "3\n",
      "10\n",
      "5\n",
      "1\n",
      "6\n",
      "3\n",
      "10\n",
      "6\n",
      "8\n",
      "5\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "10\n",
      "3\n",
      "2\n",
      "7\n",
      "7\n",
      "2\n",
      "9\n",
      "3\n",
      "9\n",
      "11\n",
      "10\n",
      "7\n",
      "2\n",
      "5\n",
      "1\n",
      "6\n",
      "10\n",
      "1\n",
      "6\n",
      "6\n",
      "10\n",
      "6\n",
      "6\n",
      "10\n",
      "1\n",
      "3\n",
      "10\n",
      "8\n",
      "7\n",
      "5\n",
      "0\n",
      "5\n",
      "1\n",
      "9\n",
      "7\n",
      "7\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "7\n",
      "10\n",
      "7\n",
      "3\n",
      "1\n",
      "8\n",
      "5\n",
      "7\n",
      "6\n",
      "10\n",
      "5\n",
      "1\n",
      "1\n",
      "9\n",
      "1\n",
      "1\n",
      "8\n",
      "8\n",
      "1\n",
      "6\n",
      "7\n",
      "1\n",
      "8\n",
      "8\n",
      "8\n",
      "10\n",
      "3\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "10\n",
      "3\n",
      "10\n",
      "10\n",
      "10\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "3\n",
      "8\n",
      "10\n",
      "9\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "5\n",
      "6\n",
      "1\n",
      "8\n",
      "7\n",
      "10\n",
      "2\n",
      "10\n",
      "4\n",
      "11\n",
      "9\n",
      "3\n",
      "0\n",
      "6\n",
      "7\n",
      "1\n",
      "0\n",
      "7\n",
      "10\n",
      "5\n",
      "7\n",
      "3\n",
      "7\n",
      "3\n",
      "7\n",
      "8\n",
      "8\n",
      "10\n",
      "3\n",
      "1\n",
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "8\n",
      "7\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "8\n",
      "7\n",
      "10\n",
      "10\n",
      "3\n",
      "6\n",
      "7\n",
      "4\n",
      "10\n",
      "1\n",
      "9\n",
      "3\n",
      "5\n",
      "8\n",
      "8\n",
      "0\n",
      "7\n",
      "7\n",
      "10\n",
      "4\n",
      "6\n",
      "1\n",
      "10\n",
      "3\n",
      "7\n",
      "6\n",
      "6\n",
      "10\n",
      "5\n",
      "1\n",
      "10\n",
      "7\n",
      "10\n",
      "8\n",
      "7\n",
      "2\n",
      "10\n",
      "11\n",
      "8\n",
      "4\n",
      "10\n",
      "1\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "1\n",
      "7\n",
      "7\n",
      "5\n",
      "8\n",
      "1\n",
      "6\n",
      "7\n",
      "2\n",
      "4\n",
      "7\n",
      "1\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "1\n",
      "6\n",
      "7\n",
      "1\n",
      "8\n",
      "8\n",
      "10\n",
      "6\n",
      "11\n",
      "10\n",
      "6\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "8\n",
      "7\n",
      "3\n",
      "1\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "2\n",
      "10\n",
      "7\n",
      "3\n",
      "10\n",
      "7\n",
      "3\n",
      "11\n",
      "5\n",
      "10\n",
      "7\n",
      "8\n",
      "1\n",
      "7\n",
      "1\n",
      "0\n",
      "0\n",
      "7\n",
      "10\n",
      "3\n",
      "11\n",
      "11\n",
      "8\n",
      "8\n",
      "1\n",
      "8\n",
      "8\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "8\n",
      "3\n",
      "3\n",
      "10\n",
      "9\n",
      "8\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "1\n",
      "7\n",
      "7\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "7\n",
      "7\n",
      "4\n",
      "3\n",
      "6\n",
      "7\n",
      "2\n",
      "10\n",
      "7\n",
      "2\n",
      "7\n",
      "7\n",
      "7\n",
      "9\n",
      "3\n",
      "3\n",
      "6\n",
      "10\n",
      "1\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "7\n",
      "1\n",
      "3\n",
      "6\n",
      "8\n",
      "10\n",
      "7\n",
      "8\n",
      "3\n",
      "3\n",
      "1\n",
      "7\n",
      "10\n",
      "10\n",
      "6\n",
      "10\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "8\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "3\n",
      "8\n",
      "10\n",
      "3\n",
      "6\n",
      "7\n",
      "3\n",
      "1\n",
      "5\n",
      "6\n",
      "7\n",
      "6\n",
      "1\n",
      "3\n",
      "6\n",
      "1\n",
      "8\n",
      "3\n",
      "7\n",
      "6\n",
      "7\n",
      "8\n",
      "6\n",
      "10\n",
      "10\n",
      "11\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "1\n",
      "10\n",
      "3\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "10\n",
      "6\n",
      "1\n",
      "8\n",
      "8\n",
      "6\n",
      "8\n",
      "10\n",
      "10\n",
      "10\n",
      "6\n",
      "8\n",
      "9\n",
      "3\n",
      "7\n",
      "1\n",
      "6\n",
      "4\n",
      "1\n",
      "1\n",
      "7\n",
      "9\n",
      "1\n",
      "6\n",
      "6\n",
      "3\n",
      "1\n",
      "7\n",
      "3\n",
      "5\n",
      "1\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "1\n",
      "10\n",
      "7\n",
      "1\n",
      "3\n",
      "6\n",
      "10\n",
      "3\n",
      "6\n",
      "10\n",
      "3\n",
      "3\n",
      "7\n",
      "6\n",
      "10\n",
      "7\n",
      "4\n",
      "3\n",
      "6\n",
      "2\n",
      "5\n",
      "1\n",
      "8\n",
      "3\n",
      "3\n",
      "3\n",
      "10\n",
      "10\n",
      "7\n",
      "6\n",
      "10\n",
      "1\n",
      "7\n",
      "10\n",
      "1\n",
      "7\n",
      "0\n",
      "7\n",
      "7\n",
      "6\n",
      "1\n",
      "1\n",
      "9\n",
      "11\n",
      "7\n",
      "10\n",
      "3\n",
      "1\n",
      "6\n",
      "3\n",
      "6\n",
      "8\n",
      "0\n",
      "6\n",
      "10\n",
      "10\n",
      "7\n",
      "8\n",
      "10\n",
      "1\n",
      "4\n",
      "5\n",
      "9\n",
      "3\n",
      "0\n",
      "1\n",
      "5\n",
      "7\n",
      "8\n",
      "6\n",
      "6\n",
      "1\n",
      "4\n",
      "8\n",
      "10\n",
      "5\n",
      "3\n",
      "7\n",
      "4\n",
      "7\n",
      "10\n",
      "1\n",
      "7\n",
      "1\n",
      "10\n",
      "6\n",
      "1\n",
      "9\n",
      "7\n",
      "3\n",
      "11\n",
      "3\n",
      "3\n",
      "3\n",
      "11\n",
      "1\n",
      "4\n",
      "6\n",
      "1\n",
      "7\n",
      "3\n",
      "8\n",
      "10\n",
      "10\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "8\n",
      "10\n",
      "11\n",
      "7\n",
      "8\n",
      "7\n",
      "11\n",
      "5\n",
      "3\n",
      "7\n",
      "8\n",
      "6\n",
      "2\n",
      "10\n",
      "1\n",
      "8\n",
      "1\n",
      "7\n",
      "10\n",
      "1\n",
      "6\n",
      "1\n",
      "7\n",
      "3\n",
      "7\n",
      "8\n",
      "1\n",
      "1\n",
      "6\n",
      "8\n",
      "3\n",
      "6\n",
      "1\n",
      "7\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "8\n",
      "6\n",
      "7\n",
      "6\n",
      "8\n",
      "6\n",
      "0\n",
      "7\n",
      "1\n",
      "0\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "8\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "9\n",
      "10\n",
      "10\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "1\n",
      "10\n",
      "6\n",
      "6\n",
      "7\n",
      "3\n",
      "10\n",
      "6\n",
      "1\n",
      "1\n",
      "7\n",
      "9\n",
      "6\n",
      "3\n",
      "7\n",
      "7\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "10\n",
      "4\n",
      "1\n",
      "8\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "1\n",
      "1\n",
      "10\n",
      "0\n",
      "8\n",
      "10\n",
      "6\n",
      "10\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "2\n",
      "7\n",
      "6\n",
      "8\n",
      "7\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "8\n",
      "10\n",
      "0\n",
      "1\n",
      "7\n",
      "3\n",
      "8\n",
      "7\n",
      "8\n",
      "10\n",
      "1\n",
      "9\n",
      "1\n",
      "7\n",
      "4\n",
      "7\n",
      "10\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "8\n",
      "2\n",
      "1\n",
      "11\n",
      "10\n",
      "9\n",
      "10\n",
      "7\n",
      "1\n",
      "3\n",
      "5\n",
      "6\n",
      "7\n",
      "1\n",
      "10\n",
      "9\n",
      "6\n",
      "6\n",
      "1\n",
      "10\n",
      "1\n",
      "1\n",
      "10\n",
      "6\n",
      "7\n",
      "3\n",
      "8\n",
      "5\n",
      "1\n",
      "4\n",
      "7\n",
      "7\n",
      "11\n",
      "3\n",
      "5\n",
      "8\n",
      "10\n",
      "10\n",
      "6\n",
      "6\n",
      "10\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "2\n",
      "8\n",
      "7\n",
      "1\n",
      "6\n",
      "1\n",
      "8\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "8\n",
      "8\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "6\n",
      "7\n",
      "2\n",
      "7\n",
      "3\n",
      "11\n",
      "1\n",
      "1\n",
      "9\n",
      "2\n",
      "7\n",
      "7\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "10\n",
      "1\n",
      "8\n",
      "8\n",
      "3\n",
      "5\n",
      "7\n",
      "11\n",
      "7\n",
      "3\n",
      "4\n",
      "8\n",
      "5\n",
      "8\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "7\n",
      "1\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "10\n",
      "10\n",
      "1\n",
      "6\n",
      "3\n",
      "6\n",
      "10\n",
      "7\n",
      "7\n",
      "9\n",
      "10\n",
      "4\n",
      "1\n",
      "5\n",
      "1\n",
      "9\n",
      "2\n",
      "3\n",
      "4\n",
      "10\n",
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "1\n",
      "7\n",
      "8\n",
      "5\n",
      "2\n",
      "7\n",
      "3\n",
      "7\n",
      "1\n",
      "10\n",
      "7\n",
      "7\n",
      "6\n",
      "7\n",
      "6\n",
      "3\n",
      "9\n",
      "4\n",
      "3\n",
      "7\n",
      "4\n",
      "3\n",
      "7\n",
      "3\n",
      "5\n",
      "10\n",
      "6\n",
      "3\n",
      "10\n",
      "10\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "10\n",
      "10\n",
      "5\n",
      "1\n",
      "1\n",
      "8\n",
      "6\n",
      "6\n",
      "11\n",
      "8\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "2\n",
      "1\n",
      "3\n",
      "7\n",
      "10\n",
      "10\n",
      "7\n",
      "6\n",
      "7\n",
      "4\n",
      "1\n",
      "4\n",
      "3\n",
      "10\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "8\n",
      "9\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "3\n",
      "10\n",
      "3\n",
      "5\n",
      "3\n",
      "10\n",
      "1\n",
      "9\n",
      "8\n",
      "8\n",
      "1\n",
      "4\n",
      "8\n",
      "7\n",
      "8\n",
      "8\n",
      "10\n",
      "7\n",
      "3\n",
      "5\n",
      "3\n",
      "6\n",
      "9\n",
      "0\n",
      "6\n",
      "8\n",
      "10\n",
      "7\n",
      "5\n",
      "7\n",
      "8\n",
      "3\n",
      "7\n",
      "7\n",
      "7\n",
      "5\n",
      "10\n",
      "2\n",
      "10\n",
      "5\n",
      "8\n",
      "6\n",
      "7\n",
      "3\n",
      "11\n",
      "6\n",
      "5\n",
      "10\n",
      "6\n",
      "3\n",
      "11\n",
      "1\n",
      "8\n",
      "7\n",
      "6\n",
      "6\n",
      "1\n",
      "8\n",
      "6\n",
      "10\n",
      "8\n",
      "4\n",
      "2\n",
      "5\n",
      "7\n",
      "7\n",
      "10\n",
      "6\n",
      "7\n",
      "6\n",
      "7\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "3\n",
      "6\n",
      "7\n",
      "1\n",
      "10\n",
      "6\n",
      "7\n",
      "6\n",
      "4\n",
      "7\n",
      "10\n",
      "8\n",
      "7\n",
      "5\n",
      "11\n",
      "8\n",
      "9\n",
      "6\n",
      "6\n",
      "0\n",
      "5\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n",
      "4\n",
      "8\n",
      "7\n",
      "1\n",
      "4\n",
      "10\n",
      "3\n",
      "6\n",
      "9\n",
      "5\n",
      "3\n",
      "8\n",
      "4\n",
      "8\n",
      "8\n",
      "3\n",
      "1\n",
      "7\n",
      "1\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "8\n",
      "8\n",
      "1\n",
      "2\n",
      "7\n",
      "7\n",
      "3\n",
      "7\n",
      "1\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "8\n",
      "6\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "7\n",
      "7\n",
      "3\n",
      "7\n",
      "7\n",
      "1\n",
      "9\n",
      "5\n",
      "10\n",
      "1\n",
      "7\n",
      "3\n",
      "4\n",
      "10\n",
      "7\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "6\n",
      "11\n",
      "6\n",
      "7\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "4\n",
      "8\n",
      "5\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "7\n",
      "7\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "7\n",
      "4\n",
      "0\n",
      "6\n",
      "5\n",
      "10\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "7\n",
      "5\n",
      "5\n",
      "10\n",
      "6\n",
      "7\n",
      "7\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "9\n",
      "10\n",
      "5\n",
      "7\n",
      "1\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "7\n",
      "3\n",
      "4\n",
      "10\n",
      "5\n",
      "7\n",
      "7\n",
      "8\n",
      "10\n",
      "8\n",
      "5\n",
      "3\n",
      "10\n",
      "10\n",
      "6\n",
      "4\n",
      "7\n",
      "6\n",
      "10\n",
      "1\n",
      "1\n",
      "2\n",
      "7\n",
      "4\n",
      "1\n",
      "10\n",
      "8\n",
      "5\n",
      "7\n",
      "3\n",
      "7\n",
      "2\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "1\n",
      "8\n",
      "3\n",
      "3\n",
      "8\n",
      "6\n",
      "7\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "6\n",
      "8\n",
      "7\n",
      "8\n",
      "5\n",
      "11\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "7\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "3\n",
      "9\n",
      "5\n",
      "10\n",
      "4\n",
      "1\n",
      "2\n",
      "7\n",
      "8\n",
      "7\n",
      "6\n",
      "7\n",
      "8\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "10\n",
      "7\n",
      "6\n",
      "8\n",
      "1\n",
      "1\n",
      "8\n",
      "1\n",
      "6\n",
      "8\n",
      "7\n",
      "2\n",
      "4\n",
      "6\n",
      "6\n",
      "10\n",
      "5\n",
      "10\n",
      "7\n",
      "10\n",
      "6\n",
      "8\n",
      "5\n",
      "7\n",
      "8\n",
      "7\n",
      "8\n",
      "7\n",
      "11\n",
      "7\n",
      "7\n",
      "7\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n",
      "8\n",
      "7\n",
      "7\n",
      "6\n",
      "8\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "8\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "7\n",
      "7\n",
      "10\n",
      "8\n",
      "6\n",
      "10\n",
      "3\n",
      "3\n",
      "6\n",
      "10\n",
      "10\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "6\n",
      "3\n",
      "1\n",
      "6\n",
      "5\n",
      "8\n",
      "7\n",
      "1\n",
      "8\n",
      "2\n",
      "6\n",
      "1\n",
      "7\n",
      "2\n",
      "7\n",
      "2\n",
      "3\n",
      "1\n",
      "8\n",
      "11\n",
      "3\n",
      "4\n",
      "9\n",
      "11\n",
      "6\n",
      "8\n",
      "7\n",
      "7\n",
      "1\n",
      "5\n",
      "8\n",
      "0\n",
      "1\n",
      "8\n",
      "7\n",
      "4\n",
      "1\n",
      "7\n",
      "7\n",
      "8\n",
      "6\n",
      "5\n",
      "4\n",
      "7\n",
      "10\n",
      "3\n",
      "3\n",
      "10\n",
      "4\n",
      "11\n",
      "6\n",
      "8\n",
      "3\n",
      "5\n",
      "1\n",
      "10\n",
      "10\n",
      "5\n",
      "1\n",
      "7\n",
      "1\n",
      "4\n",
      "10\n",
      "7\n",
      "0\n",
      "10\n",
      "8\n",
      "7\n",
      "10\n",
      "7\n",
      "1\n",
      "6\n",
      "9\n",
      "1\n",
      "8\n",
      "1\n",
      "10\n",
      "7\n",
      "8\n",
      "11\n",
      "5\n",
      "0\n",
      "7\n",
      "1\n",
      "10\n",
      "6\n",
      "7\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "6\n",
      "3\n",
      "10\n",
      "4\n",
      "9\n",
      "10\n",
      "8\n",
      "1\n",
      "7\n",
      "1\n",
      "6\n",
      "10\n",
      "7\n",
      "10\n",
      "6\n",
      "7\n",
      "7\n",
      "10\n",
      "7\n",
      "1\n",
      "3\n",
      "8\n",
      "2\n",
      "3\n",
      "8\n",
      "5\n",
      "8\n",
      "4\n",
      "7\n",
      "6\n",
      "8\n",
      "7\n",
      "5\n",
      "0\n",
      "1\n",
      "6\n",
      "6\n",
      "8\n",
      "11\n",
      "8\n",
      "7\n",
      "7\n",
      "10\n",
      "1\n",
      "10\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "10\n",
      "2\n",
      "3\n",
      "2\n",
      "8\n",
      "6\n",
      "9\n",
      "1\n",
      "6\n",
      "6\n",
      "3\n",
      "7\n",
      "5\n",
      "6\n",
      "1\n",
      "3\n",
      "7\n",
      "10\n",
      "7\n",
      "7\n",
      "1\n",
      "3\n",
      "7\n",
      "7\n",
      "6\n",
      "10\n",
      "8\n",
      "8\n",
      "7\n",
      "9\n",
      "0\n",
      "7\n",
      "8\n",
      "8\n",
      "3\n",
      "3\n",
      "7\n",
      "8\n",
      "0\n",
      "10\n",
      "6\n",
      "6\n",
      "8\n",
      "11\n",
      "3\n",
      "3\n",
      "8\n",
      "7\n",
      "8\n",
      "7\n",
      "7\n",
      "10\n",
      "3\n",
      "3\n",
      "4\n",
      "11\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "8\n",
      "1\n",
      "8\n",
      "6\n",
      "11\n",
      "7\n",
      "7\n",
      "3\n",
      "3\n",
      "9\n",
      "6\n",
      "9\n",
      "8\n",
      "10\n",
      "3\n",
      "4\n",
      "1\n",
      "10\n",
      "4\n",
      "3\n",
      "3\n",
      "8\n",
      "6\n",
      "8\n",
      "3\n",
      "7\n",
      "5\n",
      "7\n",
      "7\n",
      "8\n",
      "4\n",
      "7\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "10\n",
      "8\n",
      "5\n",
      "7\n",
      "6\n",
      "8\n",
      "8\n",
      "2\n",
      "6\n",
      "0\n",
      "10\n",
      "11\n",
      "8\n",
      "5\n",
      "6\n",
      "11\n",
      "7\n",
      "4\n",
      "1\n",
      "4\n",
      "7\n",
      "7\n",
      "1\n",
      "6\n",
      "4\n",
      "11\n",
      "1\n",
      "8\n",
      "3\n",
      "1\n",
      "10\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "8\n",
      "7\n",
      "8\n",
      "1\n",
      "6\n",
      "7\n",
      "5\n",
      "5\n",
      "1\n",
      "7\n",
      "8\n",
      "3\n",
      "10\n",
      "6\n",
      "1\n",
      "3\n",
      "7\n",
      "10\n",
      "3\n",
      "7\n",
      "7\n",
      "3\n",
      "10\n",
      "4\n",
      "11\n",
      "8\n",
      "5\n",
      "1\n",
      "8\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "10\n",
      "10\n",
      "8\n",
      "3\n",
      "7\n",
      "1\n",
      "7\n",
      "4\n",
      "1\n",
      "3\n",
      "10\n",
      "7\n",
      "3\n",
      "3\n",
      "1\n",
      "6\n",
      "1\n",
      "8\n",
      "2\n",
      "6\n",
      "5\n",
      "1\n",
      "3\n",
      "7\n",
      "6\n",
      "10\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "9\n",
      "7\n",
      "3\n",
      "7\n",
      "3\n",
      "7\n",
      "7\n",
      "1\n",
      "10\n",
      "10\n",
      "5\n",
      "11\n",
      "6\n",
      "7\n",
      "2\n",
      "7\n",
      "7\n",
      "6\n",
      "3\n",
      "6\n",
      "1\n",
      "7\n",
      "6\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "3\n",
      "3\n",
      "9\n",
      "7\n",
      "10\n",
      "7\n",
      "3\n",
      "7\n",
      "7\n",
      "4\n",
      "3\n",
      "3\n",
      "8\n",
      "7\n",
      "4\n",
      "1\n",
      "8\n",
      "7\n",
      "1\n",
      "9\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "10\n",
      "8\n",
      "10\n",
      "7\n",
      "9\n",
      "1\n",
      "1\n",
      "1\n",
      "9\n",
      "3\n",
      "6\n",
      "7\n",
      "6\n",
      "10\n",
      "4\n",
      "9\n",
      "4\n",
      "8\n",
      "7\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "7\n",
      "4\n",
      "1\n",
      "10\n",
      "6\n",
      "8\n",
      "3\n",
      "8\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "10\n",
      "7\n",
      "3\n",
      "7\n",
      "3\n",
      "5\n",
      "7\n",
      "6\n",
      "5\n",
      "7\n",
      "7\n",
      "4\n",
      "1\n",
      "7\n",
      "8\n",
      "7\n",
      "3\n",
      "6\n",
      "6\n",
      "10\n",
      "11\n",
      "5\n",
      "5\n",
      "10\n",
      "8\n",
      "1\n",
      "1\n",
      "9\n",
      "5\n",
      "7\n",
      "6\n",
      "7\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "5\n",
      "6\n",
      "9\n",
      "3\n",
      "5\n",
      "10\n",
      "7\n",
      "8\n",
      "11\n",
      "6\n",
      "1\n",
      "7\n",
      "5\n",
      "10\n",
      "6\n",
      "11\n",
      "6\n",
      "2\n",
      "7\n",
      "3\n",
      "7\n",
      "11\n",
      "7\n",
      "6\n",
      "1\n",
      "9\n",
      "6\n",
      "1\n",
      "11\n",
      "6\n",
      "8\n",
      "1\n",
      "6\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "7\n",
      "8\n",
      "0\n",
      "6\n",
      "7\n",
      "8\n",
      "11\n",
      "3\n",
      "1\n",
      "10\n",
      "0\n",
      "6\n",
      "7\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "3\n",
      "6\n",
      "1\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "10\n",
      "10\n",
      "8\n",
      "7\n",
      "8\n",
      "6\n",
      "10\n",
      "6\n",
      "11\n",
      "6\n",
      "8\n",
      "1\n",
      "1\n",
      "2\n",
      "8\n",
      "1\n",
      "8\n",
      "1\n",
      "8\n",
      "8\n",
      "1\n",
      "5\n",
      "10\n",
      "10\n",
      "1\n",
      "6\n",
      "6\n",
      "10\n",
      "7\n",
      "2\n",
      "3\n",
      "10\n",
      "4\n",
      "7\n",
      "0\n",
      "10\n",
      "6\n",
      "10\n",
      "1\n",
      "2\n",
      "1\n",
      "8\n",
      "4\n",
      "8\n",
      "3\n",
      "3\n",
      "7\n",
      "1\n",
      "3\n",
      "10\n",
      "7\n",
      "7\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "7\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "10\n",
      "0\n",
      "10\n",
      "7\n",
      "10\n",
      "4\n",
      "4\n",
      "10\n",
      "5\n",
      "7\n",
      "3\n",
      "1\n",
      "7\n",
      "10\n",
      "6\n",
      "6\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "8\n",
      "8\n",
      "9\n",
      "7\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "4\n",
      "7\n",
      "9\n",
      "6\n",
      "3\n",
      "3\n",
      "6\n",
      "1\n",
      "7\n",
      "1\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "11\n",
      "6\n",
      "3\n",
      "8\n",
      "7\n",
      "3\n",
      "10\n",
      "9\n",
      "6\n",
      "8\n",
      "6\n",
      "3\n",
      "5\n",
      "1\n",
      "9\n",
      "7\n",
      "1\n",
      "7\n",
      "7\n",
      "5\n",
      "3\n",
      "10\n",
      "2\n",
      "8\n",
      "10\n",
      "8\n",
      "9\n",
      "5\n",
      "5\n",
      "6\n",
      "8\n",
      "6\n",
      "10\n",
      "11\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "10\n",
      "8\n",
      "10\n",
      "1\n",
      "8\n",
      "1\n",
      "1\n",
      "8\n",
      "9\n",
      "7\n",
      "3\n",
      "7\n",
      "7\n",
      "3\n",
      "1\n",
      "1\n",
      "11\n",
      "6\n",
      "7\n",
      "6\n",
      "3\n",
      "7\n",
      "5\n",
      "5\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "8\n",
      "8\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "1\n",
      "5\n",
      "10\n",
      "3\n",
      "7\n",
      "3\n",
      "5\n",
      "4\n",
      "1\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "10\n",
      "10\n",
      "11\n",
      "8\n",
      "6\n",
      "7\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "7\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "10\n",
      "8\n",
      "6\n",
      "6\n",
      "6\n",
      "10\n",
      "11\n",
      "4\n",
      "4\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "1\n",
      "10\n",
      "3\n",
      "7\n",
      "8\n",
      "6\n",
      "4\n",
      "3\n",
      "1\n",
      "6\n",
      "7\n",
      "7\n",
      "5\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "3\n",
      "10\n",
      "7\n",
      "7\n",
      "1\n",
      "3\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "7\n",
      "10\n",
      "6\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Preparing test data for final predictions\n",
    "\n",
    "lines_pred = [line.rstrip('\\n') for line in open(\"/home/shringi/Downloads/offline_challenge/offline_challenge/xtest_obfuscated.txt\")]\n",
    "# with open(\"/home/dupree/Downloads/offline_challenge/offline_challenge/xtrain_obfuscated.txt\") as f:\n",
    "#     lines = f.readlines()\n",
    "# random.shuffle(lines)\n",
    "pred_data_x = lines_pred\n",
    "len(pred_data_x)\n",
    "xpred_seq = token.texts_to_sequences(pred_data_x)\n",
    "\n",
    "# zero pad the sequences\n",
    "xpred_pad = sequence.pad_sequences(xpred_seq, maxlen=max_len)\n",
    "\n",
    "print len(xpred_pad)\n",
    "# print xpred_pad[0]\n",
    "# result = model.predict(xpred_pad)\n",
    "# print result[:5]\n",
    "predictions = model.predict_classes(xpred_pad)\n",
    "print predictions[:5]\n",
    "print (predictions).shape\n",
    "\n",
    "for class_label in predictions:\n",
    "    print class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/100\n",
      "26010/26010 [==============================] - 4s - loss: 2.2635 - acc: 0.1896 - val_loss: 2.1370 - val_acc: 0.2470\n",
      "Epoch 2/100\n",
      "26010/26010 [==============================] - 4s - loss: 1.8503 - acc: 0.3499 - val_loss: 1.5993 - val_acc: 0.4215\n",
      "Epoch 3/100\n",
      " 2752/26010 [==>...........................] - ETA: 3s - loss: 1.5537 - acc: 0.4371"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6d1443a82661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#           verbose=1, validation_data=(xvalid_glove_scl, yvalid_enc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=100, \n\u001b[0;32m---> 65\u001b[0;31m           verbose=1, validation_data=(xvalid_pad, yvalid_enc))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shringi/anaconda2/envs/dupree/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using a Convolution1D, with GloVe features\n",
    "# # scale the data before any neural net:\n",
    "# scl = preprocessing.StandardScaler()\n",
    "# xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "# xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "# model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Dense(300, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Dense(12))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# Training a 1D convnet with existing GloVe features/vectors\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128,\n",
    "                 5,\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           epochs=2, batch_size=128)\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
